<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>AI Pocket Reference: NLP</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="A streamlined reference manual for AI practitioners, students, and developers to quickly look up core concepts and mock implementations">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../_common/mdbook-ai-pocket-reference.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">AI Pocket Reference: NLP</h1>

                    <div class="right-buttons">
                        <a href="https://vectorinstitute.github.io/ai-pocket-reference/" title="Home" aria-label="Home">
                            <i id="home-button" class="fa fa-home"></i>
                        </a>
                        <a href="https://github.com/VectorInstitute/ai-pocket-reference" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Welcome to AI Pocket References: NLP Collection. This compilation covers a broad
range of Natural Language Processing topics including foundational LLM concepts,
architectures, prompting techniques, fine-tuning approaches, and evaluation metrics.
These concise references are designed for quick understanding and practical application.</p>
<p>Be sure to check out our other collections of <a href="https://vectorinstitute.github.io/ai-pocket-reference/">AI Pocket References!</a></p>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="chain-of-thought"><a class="header" href="#chain-of-thought">Chain of Thought</a></h1>
<!-- Header -->
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 3 min</small></p>
  </div>
</div>
<!-- Main Body -->
<p>The Chain of Thought (CoT) prompting technique, introduced by Wei, Jason et al (2022),
encourages an LLM to articulate its reasoning steps before arriving at a final
answer to a given task.</p>
<p>Before its introduction, scaling LLMs had demonstrated the ability to generate coherent
text and solve various tasks. However, these LLMs still underperformed on complex
reasoning tasks like arithmetic and symbolic reasoning. While some prompting techniques
and <a href="llms/prompting/./icl.html">in-context learning</a> had already been discovered, none had successfully
enabled LLMs to handle complex reasoning tasks.</p>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/nlp/cot.svg" alt="cot"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: LLM producing a chain of thought.
</div>
<h2 id="original-implementation-details"><a class="header" href="#original-implementation-details">Original Implementation Details</a></h2>
<p>CoT was originally introduced as a few-shot prompting technique where each included
exemplar is augmented with a <em>chain of thought</em> that explains how the final answer
was determined. An example of such an exemplar taken from the original paper is
provided below:</p>
<pre><code class="language-yaml"># An exemplar
exemplar:
  question: &gt;
    Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each
    can has 3 tennis balls. How many tennis balls does he have now?
  chain of thought: &gt;
    Roger started with 5 balls. 2 cans of 3 tennis balls each
    is 6 tennis balls. 5 + 6 = 11.
  answer: The answer is 11.
</code></pre>
<p>The authors used the same set of 8 exemplars across all tested benchmarks, with
the exception of <a href="https://github.com/google-deepmind/AQuA">AQuA</a>, for which 4
exemplars derived from the training set were used instead."</p>
<h2 id="performance"><a class="header" href="#performance">Performance</a></h2>
<p>With larger models, CoT outperformed standard prompting across all tested reasoning
benchmarks (mathematical, commonsense, and symbolic). For some of these, it even
achieved state of the art results, beating out previous methods that relied on
fine-tuning. However, CoT added little benefit for smaller models, leading the
authors to posit it as an <a href="llms/prompting/../misc/emergent.html">emergent</a> ability of model
scale.</p>
<h2 id="limitations"><a class="header" href="#limitations">Limitations</a></h2>
<p>One of the noted limitations of CoT is the lack of guarantees on correct reasoning
paths taken by the LLM. In other words, the reasoning steps that the LLM performs
can be flawed, leading to inefficient token generation and potentially amplifying
the issue of LLM hallucinations.</p>
<h2 id="modern-implementations"><a class="header" href="#modern-implementations">Modern Implementations</a></h2>
<p>Since its introduction, the CoT prompting technique has become more flexible.
Broadly speaking, it is widely recognized as a prompting technique that elicits a
<em>chain of thought</em> output in its generation. To do so, many include general
instructions in the prompt, specifying the desired output format and other requirements.
With these system instructions and output formats, CoT can also be implemented
in a zero-shot fashion.</p>
<pre><code class="language-yaml"># Example CoT prompt instructions
prompt:
  system: &gt;
    You are a helpful assistant that is able to handle complex reasoning
    tasks. To arrive at the final answer, perform chain of thought steps
    and include these in your output.

    Structure your output using the following format
      &lt;thought&gt;
        chain of thought here
      &lt;/thought&gt;
      &lt;answer&gt;
        answer here
      &lt;/answer&gt;
  question: &gt;
    Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each
    can has 3 tennis balls. How many tennis balls does he have now?
</code></pre>
<h4 id="references--useful-links"><a class="header" href="#references--useful-links">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2201.11903"><em>Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large
language models." Advances in neural information processing systems 35 (2022):
24824-24837.</em></a></li>
</ol>
<!-- Contributions -->
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="lora"><a class="header" href="#lora">LoRA</a></h1>
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <a target="_blank" href="https://colab.research.google.com/github/VectorInstitute/ai-pocket-reference-code/blob/main/notebooks/nlp/lora.ipynb">
      <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 3 min</small></p>
  </div>
</div>
<p>Low-rank adaptation (LoRA) is parameter-efficient fine-tuning (<a href="llms/fine_tuning/../fine_tuning/peft.html">PEFT</a>)
introduced by Hu, Edward J. et al. (2021). The creators of LoRA posited that since
trained deep learning models reside in low intrinsic dimensions, perhaps their
weight-update matrices do as well.</p>
<p>Specifically, with LoRA, we learn a low-rank representation of the weight-update
matrices of dense, linear layers of a pre-trained LLM. The original weights
of the LLM are frozen during fine-tuning and only the low-rank weight-update matrices
at each step of fine-tuning. This reduction in dimensionality helps to amplify the
most important or influential features of the model.</p>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/nlp/LoRA.svg" alt="lora"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: Illustrating a forward pass with LoRA
</div>
<h2 id="some-math"><a class="header" href="#some-math">Some Math</a></h2>
<p>Let \(W\) represent the \(d\times d\) weight matrix for a dense, linear layer.
We can then loosely represent an updated version (i.e. after fine-tuning) of
this matrix as follows:</p>
<p>$$W_{\text{updated}} = W + \Delta W,$$</p>
<p>where \(\Delta W\) is the update matrix. With LoRA, it is \(\Delta W\) which
we project into a low-rank space:</p>
<p>$$\Delta W \approx AB,$$</p>
<p>where \(A\) and \(B^T\) are both matrices of dimension \(d \times r\) and
\(r &lt;&lt; d\). During fine-tuning, \(W\) is frozen and only \(A\) and \(B\)
are updated.</p>
<p>For inference (i.e., forward phase), let \(x\) be an input embedding, then by
the distributive property</p>
<p>$$xW_{\text{updated}} = xW + x\Delta W \approx xW + xAB.$$</p>
<h2 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h2>
<p>One modular implementation of LoRA involves the introduction of a <code>LoRALayer</code> that
comprises of only the \(A\) and \(B\) dense weights. In this way, a <code>LoRALayer</code>
can adapt a pre-trained <code>Linear</code> layer.</p>
<pre><code class="language-python">import torch


class LoRALayer(torch.nn.Module):
    """A basic LoRALayer implementation."""

    def __init__(self, d_in: int, d_out: int, rank: int):
        self.A = torch.nn.Parameter(torch.empty(d_in, rank))
        self.B = torch.nn.Parameter(torch.zeros(rank, d_out))

    def forward(self, x):
        return x @ self.A @ self.B
</code></pre>
<p>With the <code>LoRALayer</code> defined in this way, one can then combine this with a <code>Linear</code>
layer to implement the LoRA technique. See the supplementary Colab notebook linked
at the top of this pocket reference for more details.</p>
<h2 id="performance-1"><a class="header" href="#performance-1">Performance</a></h2>
<p>In the original paper, the authors reported similar levels of performance when using
LoRA compared to full fine-tuning on various natural language generation and understanding
tasks.</p>
<h2 id="additional-benefits"><a class="header" href="#additional-benefits">Additional Benefits</a></h2>
<p>Since LoRA matrices can be stored efficiently and separately from the pre-trained
LLM weights, customization of these large models is highly scalable. Organizations
can build libraries of specialized LoRA matrices for different datasets and domains,
switching between them as needed for specific applications.</p>
<h2 id="limitations-1"><a class="header" href="#limitations-1">Limitations</a></h2>
<h4 id="references--useful-links-1"><a class="header" href="#references--useful-links-1">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2106.09685"><em>Hu, Edward J., et al. "Lora: Low-rank adaptation of large language models."
arXiv preprint arXiv:2106.09685, 2021.</em></a></li>
<li><a href="https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167"><em>Raschka, Sebastian. Build a Large Language Model (From Scratch). Simon and
Schuster, 2024.</em></a></li>
<li><a href="https://huggingface.co/docs/peft/en/task_guides/lora_based_methods"><em>Sourab Mangrulkar et al. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods (LoRA methods), 2022.</em></a></li>
<li><a href="https://arxiv.org/pdf/2307.13269"><em>Huang, Chengsong, et al. "Lorahub: Efficient cross-task generalization via
dynamic lora composition." arXiv preprint arXiv:2307.13269 (2023).</em></a></li>
<li><a href="https://d3ddy8balm3goa.cloudfront.net/paper-cards/w29_2023-lora.excalidraw.svg"><em>Fajardo V.A. LoRA PaperCard, 2023.</em></a></li>
</ol>
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="agents"><a class="header" href="#agents">Agents</a></h1>
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 5 min</small></p>
  </div>
</div>
<p>Agents are LLMs equipped with <a href="llms/agents/./tool_use.html">tools</a> and memory to interact
with the environment and complete specific, user-defined objectives.
They go about this by following workflows which direct them in
(i) <a href="llms/agents/./planning.html">planning</a> for what steps and tools are needed,
(ii) executing an action, and (iii) <a href="llms/agents/./reflection.html">reflecting</a> on feedback
from their action, looping through these steps when the initial plan requires
multiple actions, or when reflection suggests additional actions are needed to
achieve the objective.
Compared to an LLM on its own, the "plan-action-reflect" workflow of agents
give them a higher degree of agency and capacity for complex or long term
tasks, while tools offer the ability to learn up-to-date information
from the environment and offload certain computations.</p>
<h2 id="components"><a class="header" href="#components">Components</a></h2>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/nlp/agent.svg" alt="agent"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: Agent components
</div>
<h3 id="tools"><a class="header" href="#tools">Tools</a></h3>
<p><a href="llms/agents/./tool_use.html">Tools</a> can include external data sets (including
unstructured data such as PDF documents), web searches, APIs, custom functions,
and even <a href="llms/agents/./multi_agents.html">other agents</a>.
They should fulfill a clear objective which is clearly communicated
to the LLM through a short, formatted description.
This is so the LLM is aware of the tool's existence and can invoke it
when necessary. However, since an LLM's input and output are text-based,
invoking a tool means generating a structured output in a specific
format—typically JSON or direct code. Standardized communication
between tools and LLMs are also being established and propagated, with
Anthropic's Model Context Procotol <a href="https://www.anthropic.com/news/model-context-protocol">(2024)</a>
being a notable example.</p>
<h3 id="memory"><a class="header" href="#memory">Memory</a></h3>
<p>The information retrieved from a tool updates the agent's memory,
which also contains the context of its objective: the original
user-defined objective, any other user input, as well as the results
of prior planning, actions, and reflection.
This memory is vital for the agent to complete its objective coherently
and not be stuck in endless loops, conducting unnecessary actions,
or offering irrelevant results. Memory does not have to be one continuous
block. It can be separated into multiple sections with different persistence
and frequencies of use. This is the case with LlamaIndex's
<a href="https://docs.llamaindex.ai/en/stable/examples/agent/memory/composable_memory/">composable memory</a>.</p>
<h3 id="llm"><a class="header" href="#llm">LLM</a></h3>
<p>The underlying LLM powering the agent's every move can be
any language model, including the notable models listed in
<a href="llms/agents/../../models/index.html">the pocket references of notable models</a>
(e.g., <a href="llms/agents/../../models/llama_3.html">Llama-3</a>,
<a href="llms/agents/../../models/deepseek_r1.html">DeepSeek-R1</a>, etc.).</p>
<h3 id="framework"><a class="header" href="#framework">Framework</a></h3>
<p>The final component of an agent that cannot be overlooked is the
"connective tissue" that enables the LLM to work together with its memory
and its tools. This is the code that structures the user-defined objective
and the tool descriptions into a prompt for the LLM, that parses an LLM's
output and directs their JSON/code to the corresponding tool, that incorporates
tool results into memory and text to properly give back to
the LLM for reflection. Some popular frameworks include <a href="https://github.com/crewAIInc/crewAI">CrewAI</a>,
<a href="https://github.com/geekan/MetaGPT">MetaGPT</a>,
<a href="https://github.com/huggingface/smolagents">smolagents</a>,
<a href="https://github.com/langchain-ai/langgraph">LangGraph</a>,
<a href="https://github.com/run-llama/llama_index">LlamaIndex</a>.</p>
<h2 id="applications"><a class="header" href="#applications">Applications</a></h2>
<p>The potential applications for agents are quite broad. Some examples include:</p>
<ul>
<li>Personal scheduling assistant</li>
<li>Customer service queue specialist</li>
<li>Internet-of-things (IoT) hub manager</li>
<li>Discussion forum moderator</li>
<li>Lab research assistant</li>
<li>etc.</li>
</ul>
<p>While the above are theoretical applications, tangible agents have also begun
making their way to the market. At the time of writing this reference,
Deep Research <a href="https://openai.com/index/introducing-deep-research/">(2025)</a>,
a research assistant for synthesizing literature,
and Manus <a href="https://manus.im/">(2025)</a>,
for broader analysis and development,
are two agents that have made headlines. Furthermore, Casper et al. <a href="https://arxiv.org/pdf/2502.01635">(2025)</a>
have compiled an <a href="https://aiagentindex.mit.edu/index/">index</a>
to track existing agents and discover patterns. They
have found agents "being deployed at a steadily increasing rate".</p>
<h2 id="limitations-2"><a class="header" href="#limitations-2">Limitations</a></h2>
<p>Built on LLMs, agents can be computationally expensive,
and thus may be unnecessary for overly simplistic tasks.
Due to their semi-supervised nature, the possibility of an agent
making inefficient LLM calls or worse, getting stuck in feedback loops,
should be kept in mind.
Extra care should be taken when implementing multi-agent collaboration,
as a hallucination in one agent would affect the whole system.
Additionally with multi-agents, if they were all built on the same LLM,
any reasoning deficiency would be shared across all agents.
Lastly, more work and time are needed to foster community trust
in the viability of agents for everyday life.</p>
<p>Advances have been made to address these perceived shortcomings,
with the foremost technique being <a href="llms/agents/../fine_tuning/index.html">fine-tuning</a>.
Supervised fine-tuning with instructions
<a href="https://arxiv.org/abs/2308.10792">(Zhang et al., 2024)</a>,
alignment and safety fine-tuning
<a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives">(Raschka, 2023)</a>
, and reasoning fine-tuning with reinforcement learning
<a href="https://arxiv.org/abs/2401.08967">(Luong et al., 2024)</a>,
are among the promising fine-tuning techniques proposed.</p>
<h2 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h2>
<p>While there is general agreement on what agents do in the industry,
the specifics vary between sources and application settings. Therefore,
it would be useful to browse different definitions and see more perspectives.
The links below include introductions to agents from
<a href="https://huggingface.co/learn/agents-course/unit1/what-are-agents">Hugging Face</a>
,
<a href="https://github.com/nerdai/talks/blob/main/2024/genai-philippines/genai-philippines.ipynb">LlamaIndex</a>
,
<a href="https://www.ibm.com/think/topics/ai-agents">IBM</a>
,
<a href="https://aiagentindex.mit.edu/">MIT</a>
,
and
<a href="https://www.anthropic.com/engineering/building-effective-agents">Anthropic</a>.</p>
<h4 id="references--useful-links-2"><a class="header" href="#references--useful-links-2">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://www.anthropic.com/news/model-context-protocol"><em>Anthropic. "Introducing the Model Context Protocol." Anthropic News,
anthropic.com/news/model-context-protocol. Accessed 14 Mar. 2025.</em></a></li>
<li><a href="https://docs.llamaindex.ai/en/stable/examples/agent/memory/composable_memory/"><em>LlamaIndex. "Single composable memory." LlamaIndex Documentation,
docs.llamaindex.ai/en/stable/examples/agent/memory/composable_memory/.
Accessed 14 Mar. 2025.</em></a></li>
<li><a href="https://openai.com/index/introducing-deep-research/"><em>OpenAI. "Introducing deep research." OpenAI Documentation,
openai.com/index/introducing-deep-research/. Accessed 8 Mar. 2025.</em></a></li>
<li><a href="https://manus.im/"><em>Manus. "Introducing Manus." Manus Home Page, manus.im/.
Accessed 8 Mar. 2025.</em></a></li>
<li><a href="https://arxiv.org/pdf/2502.01635"><em>Casper, S., Bailey, L., Hunter, R., Ezell, C., Cabalé, E.,
Gerovitch, M., Slocum, S., Wei, K., Jurkovic, N., Khan, A.,
Christoffersen, P.J.K., Ozisik, A.P., Trivedi, R., Hadfield-Menell, D.,
and Kolt, N. "The AI Agent Index." (2025),
DOI: 10.48550/arXiv.2502.01635.</em></a></li>
<li><a href="https://arxiv.org/abs/2308.10792"><em>Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R.,
Zhang, T., Wu, F., and Wang, G. "Instruction tuning for large
language models: a survey." (2024),
DOI: 10.48550/arXiv.2308.10792.</em></a></li>
<li><a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives"><em>Raschka, S. "LLM training: RLHF and its alternatives." (2023),
Ahead of AI,
magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives.
Accessed 14, Mar. 2025.</em></a></li>
<li><a href="https://arxiv.org/abs/2401.08967"><em>Luong, T.Q., Zhang, X., Jie, Z., Sun, P., Jin, X., and Li, H.
"ReFT: reasoning with reinforced fine-tuning." (2024),
DOI: doi.org/10.48550/arXiv.2401.08967.</em></a></li>
<li><a href="https://huggingface.co/learn/agents-course/unit1/what-are-agents"><em>Hugging Face. "What is an agent?" Hugging Face Agents Course,
huggingface.co/learn/agents-course/unit1/what-are-agents.
Accessed 8 Mar. 2025.</em></a></li>
<li><a href="https://github.com/nerdai/talks/blob/main/2024/genai-philippines/genai-philippines.ipynb"><em>Fajardo, V.A. LlamaIndex GenAI Philippines talk, github.com/nerdai/
talks/blob/main/2024/genai-philippines/genai-philippines.ipynb.
Accessed 8 Mar. 2025.</em></a></li>
<li><a href="https://www.ibm.com/think/topics/ai-agents"><em>IBM. "What are AI agents?" IBM Think, ibm.com/think/topics/ai-agents.
Accessed 8 Mar. 2025.</em></a></li>
<li><a href="https://www.anthropic.com/engineering/building-effective-agents"><em>Anthropic. "Building effective agents." Anthropic Engineering,
anthropic.com/engineering/building-effective-agents.
Accessed 14 Mar. 2025.</em></a></li>
</ol>
<h4 id="github-repositories-for-agent-frameworks"><a class="header" href="#github-repositories-for-agent-frameworks">GitHub Repositories for Agent Frameworks</a></h4>
<ol>
<li><a href="https://github.com/crewAIInc/crewAI"><em>crewAI. github.com/crewAIInc/crewAI</em></a></li>
<li><a href="https://github.com/geekan/MetaGPT"><em>MetaGPT. github.com/geekan/MetaGPT</em></a></li>
<li><a href="https://github.com/huggingface/smolagents"><em>smolagents. github.com/huggingface/smolagents</em></a></li>
<li><a href="https://github.com/langchain-ai/langgraph"><em>LangGraph. github.com/langchain-ai/langgraph</em></a></li>
<li><a href="https://github.com/run-llama/llama_index"><em>LlamaIndex. github.com/run-llama/llama_index</em></a></li>
</ol>
<!-- Contributions -->
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/c567wang">
<img src="https://github.com/c567wang.png"
  width="32px" alt="Contributor c567wang" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="rag"><a class="header" href="#rag">RAG</a></h1>
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <a target="_blank" href="https://colab.research.google.com/github/VectorInstitute/ai-pocket-reference-code/blob/main/notebooks/nlp/rag.ipynb">
      <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 7 min</small></p>
  </div>
</div>
<h2 id="intro-and-motivation-for-rag"><a class="header" href="#intro-and-motivation-for-rag">Intro and Motivation for RAG</a></h2>
<p>After an LLM has been pre-trained, its learning is captured in <em>parametric</em> knowledge.
This speak is jargon simply implying that the knowledge is captured in the LLM's
weight parameters. If the LLM is further fine-tuned for improved instruction following
or alignment, these knowledge specializations are parametric in nature (i.e.,
since these involve weight parameter updates).</p>
<p>However, researchers have observed that relying only on the LLM's parametric knowledge,
can be suboptimal and this is especially observed when performing knowledge-intensive
tasks. Some pundits have argued that long-tail knowledge is not easily captured
in parametric form.</p>
<p>To remedy this drawback of an LLM's parametric knowledge, we can consider providing
an LLM with non-parametric knowledge. Retrieval-Augmented Generation (RAG) is one
such technique that aims to provide knowledge in the form of additional context
to an LLM at inference time. As it's name suggests, this method involves
retrieving facts (i.e., knowledge) from a data store and augmenting (e.g., by
string concatenation) the original prompt or query to the LLM with these facts.</p>
<h2 id="components-of-a-rag-system"><a class="header" href="#components-of-a-rag-system">Components of a RAG System</a></h2>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/nlp/rag-components.excalidraw.svg" alt="rag-components"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: The components of RAG.
</div>
<p>A RAG system is comprised of three main components, namely:</p>
<ul>
<li><strong>Knowledge Store</strong> — contains non-parametric knowledge facts that the system
can use at inference time in order to produce more accurate responses to queries.</li>
<li><strong>Retriever</strong> — a model that takes in a user query and retrieves the most relevant
knowledge facts from the knowledge store. (NOTE: the retriever is also used to
populate or index the knowledge store during setup.)</li>
<li><strong>Generator</strong> — a model that takes in the user's query and additional context
and provides a response to that query.</li>
</ul>
<h2 id="canonical-rag-pipeline"><a class="header" href="#canonical-rag-pipeline">Canonical RAG Pipeline</a></h2>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/nlp/rag-message-flow.excalidraw.svg" alt="rag-message-flow"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: Depicting the canonical RAG message flow diagram. A user submits a query
to the RAG system, which ultimately returns the response back to the user only after
completing both retrieval and generation steps in sequence.
</div>
<p>The canonical pipeline for RAG is as follows:</p>
<ol>
<li>User submits a query to the RAG system</li>
<li><strong>[Retrieval Step]</strong> The RAG system matches the query with the relevant facts
from the knowledge store. The top k matched facts are retrieved.</li>
<li><strong>[Generation Step]</strong> The content of the retrieved facts are used to augment the
query and subsequently pass to the generator.</li>
<li>Response is returned back to the user. (Post-processing steps may be applied
to the raw result from the generator prior to returning it to the user.)</li>
</ol>
<h2 id="evaluation-of-rag-systems"><a class="header" href="#evaluation-of-rag-systems">Evaluation of RAG Systems</a></h2>
<p>Evaluation of RAG systems is often not a trivial task. A common way that these
systems are evaluated are by the evaluation of the respective components, namely:
retriever and generator evaluation.</p>
<h3 id="evaluation-of-retriever"><a class="header" href="#evaluation-of-retriever">Evaluation of Retriever</a></h3>
<p>Retriever's are evaluated based on the correctness of the retrieved facts. Given
a "labelled" example containing the query as well as associated facts, we can compute
metrics such as hit rate and normalized discounted cumulative gain (NDCG). The
former computes the fraction of retrievals that returned the correct knowledge
artifact over the number of queries (or retrieval tasks). While hit rate doesn't
take into account the order in which knowledge facts are retrieved, NDCG incorporates
this ordering in its calculation, considering retrievals successful when the correct
knowledge artifacts appear in the highest-ranked positions.</p>
<h3 id="evaluation-of-generator"><a class="header" href="#evaluation-of-generator">Evaluation of Generator</a></h3>
<p>Generator responses can be done via human scoring where a human assess the response
to the query given the context. The human can provide a numerical score to indicate
how well the generator answers the query with the provided context. Metrics such
as faithfulness and accuracy are often computed. However, human marking is expensive
and thus another strategy makes use of LLMs (i.e., LLM As A Judge) to perform the
grading.</p>
<h2 id="limitations-3"><a class="header" href="#limitations-3">Limitations</a></h2>
<p>While RAG has demonstrated success in providing LLMs with sufficient context in
order to perform well across various knowledge-intensive benchmarks, building a
RAG system involves many system-level parameters, and tuning these to achieve
sufficient performance is non-trivial.</p>
<p>Examples of these system-level parameters include:</p>
<ul>
<li><em>On representing knowledge (i.e., knowledge store setup)</em>
<ul>
<li><strong>chunk size</strong> — when populating the knowledge store, texts are chunked in
order to ensure that queries along with context are within the context windows
of the LLM generator</li>
<li><strong>hierarchical representations</strong> — knowledge facts may depend on one another
or may contain levels of hierarchy that should be captured in the knowledge store.
Advance knowledged representations via knowledge graphs are also an option but
come with its own challenges to reach a satisfactory performance (i.e., how to
setup the knowledge graph optimally).</li>
</ul>
</li>
<li><em>On retrieval</em>
<ul>
<li><strong>matching query to knowledge facts</strong> — the raw user query may need some
processing in order to increase the chances of finding relevant facts from the
knowledge store. (e.g., query re-write or agentic planning)</li>
</ul>
</li>
<li><em>On generation</em>
<ul>
<li><strong>hallucinations</strong> — in the event that there are no retrieved facts, there are
still risks for LLM hallucinations.</li>
</ul>
</li>
</ul>
<h2 id="advanced-techniques"><a class="header" href="#advanced-techniques">Advanced Techniques</a></h2>
<p>In this section, we present a few advanced techniques for building RAG systems.
Generally speaking, advanced methods aim to address the two main requirements
for success of a RAG system, namely:</p>
<ol>
<li>Retrieval must be able to find the most relevant knowledge facts for the user
query.</li>
<li>Generation must be able to make good use of the retrieved knowledge facts.</li>
</ol>
<p>Advanced techniques can be viewed as addressing one of these requirements or both
simultaneously. Examples include individual fine-tuning of embedding or LLM model
in order improve retrieval and generation, alone. However, dual fine-tuning of
these can be considered to address both requirements simultaneously. See the
cheat sheet below for more advanced RAG designs.</p>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/llamaindex/rag-cheat-sheet-final.svg" alt="rag-cheat-sheet"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: A RAG cheat sheet displaying various techniques for building advanced
retrieval-augmented generation systems. The left side shows methods for
independently addressing generation requirements (including compression, re-ranking,
and adapter methods), while the right side illustrates techniques for simultaneously
addressing multiple requirements (including fine-tuning, foundational models,
and iterative retrieval-generation). (Created by author while working at
LlamaIndex, 2024.)
</div>
<h2 id="frameworks"><a class="header" href="#frameworks">Frameworks</a></h2>
<h3 id="rag-inference-frameworks"><a class="header" href="#rag-inference-frameworks">RAG Inference Frameworks</a></h3>
<p>There are a handful of popular open-source frameworks that exist that help to
build RAG systems on top of the user's own data sources. These frameworks are
useful for quick proto-typing of RAG systems, supporting both basic and advanced
designs. Another major advantage of these frameworks is their vast integrations
to other tools in the tech stack i.e., vector stores, llm providers (both closed
and open options), embedding models, observability tools, etc.</p>
<p>Three popular frameworks for RAG include:</p>
<ol>
<li>LlamaIndex - <a href="https://github.com/run-llama/llama_index">https://github.com/run-llama/llama_index</a></li>
<li>LangChain - <a href="https://github.com/langchain-ai/langchain">https://github.com/langchain-ai/langchain</a></li>
<li>Haystack by Deepset - <a href="https://github.com/deepset-ai/haystack">https://github.com/deepset-ai/haystack</a></li>
</ol>
<h3 id="rag-finetuning-frameworks"><a class="header" href="#rag-finetuning-frameworks">RAG Finetuning Frameworks</a></h3>
<p>The previous section mentioned frameworks that are effective for building RAG inference
systems. For fine-tuning RAG, under both centralized and federated settings, the
Vector Institute has developed, fedRAG:
<a href="https://github.com/VectorInstitute/fed-rag">https://github.com/VectorInstitute/fed-rag</a>.</p>
<p>The fedRAG framework features a lightweight
interface to turn a centralized model training pipeline into a federated task.
Additionally, it boasts integrations to popular deep-learning tools and frameworks
including PyTorch and HuggingFace.</p>
<h4 id="references--useful-links-3"><a class="header" href="#references--useful-links-3">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://github.com/run-llama/llama_index"><em>Liu, Jerry. "LlamaIndex." GitHub, Nov. 2022. DOI: 10.5281/zenodo.1234.
github.com/run-llama/llama_index</em></a></li>
<li><a href="https://medium.com/llamaindex-blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"><em>A Cheat Sheet and Some Recipes For Building Advanced RAG." LlamaIndex Blog,
Andrei Fajardo, 5 Jan. 2024, medium.com/llamaindex-blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag</em></a>.</li>
<li><a href="https://github.com/VectorInstitute/rag_bootcamp"><em>Rag Bootcamp. GitHub, Vector Institute, github.com/VectorInstitute/rag_bootcamp</em></a>.</li>
<li><a href="https://arxiv.org/pdf/2005.11401"><em>Lewis, Patrick, et al. "Retrieval-augmented generation for knowledge-intensive
nlp tasks." Advances in neural information processing systems 33 (2020): 9459-947</em></a></li>
</ol>
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quantization"><a class="header" href="#quantization">Quantization</a></h1>
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 4 min</small></p>
  </div>
</div>
<h2 id="1-introduction-to-quantization"><a class="header" href="#1-introduction-to-quantization">1. Introduction to Quantization</a></h2>
<p>Quantization reduces the numerical precision of the weights and activations of
a neural network from a high-precision datatype to a lower-precision datatype.
The standard datatype for neural network weights and activations is fp32 (float32),
but quantization methods enable use of lower-precision representations, most commonly
8-bit integers (int8) and 16-bit floats (float16 or bfloat16). Even 4-bit integers
see use. Think of it like compressing a high-definition image into a
lower-resolution format – you lose some detail, but gain efficiency.</p>
<p><strong>Why Quantization?</strong></p>
<ul>
<li><strong>Lower Memory Requirements:</strong> Reduced bit-depth directly translates to smaller
model memory footprint, saving memory capacity and bandwidth during model storage
and inference. This also makes it feasible to deploy larger, more performant models
on resource-constrained devices.</li>
<li><strong>Faster Throughput:</strong> Lower precision is significantly faster and more energy-efficient
on modern hardware. Modern GPUs with tensor cores (like NVIDIA's Ampere and Hopper
architectures) can perform low and mixed-precision matrix multiplications with
significantly higher throughput than FP32.</li>
</ul>
<h2 id="2-how-quantization-works"><a class="header" href="#2-how-quantization-works">2. How Quantization Works</a></h2>
<p>This mapping is typically done using <em>affine quantization</em> or <em>symmetric quantization</em>.
For example, an affine quantization from fp32 to int8 involves two parameters for
each tensor being quantized:</p>
<ul>
<li><strong>Scale (S):</strong> A positive floating-point number that determines the step size
between quantized integer values. It essentially scales the integer range back
to the floating-point range.</li>
<li><strong>Zero-point (Z):</strong> An integer that represents the floating-point value 0 in
the quantized integer space. This is important for accurately representing zero
values, which are common in neural networks (e.g., after ReLU activations).</li>
</ul>
<p>The relationship between a floating-point value (\(x\)) and its quantized
integer representation (\(x_q\)) is defined by:</p>
<p>$$x = S (x_q - Z).$$</p>
<p>To quantize a float value \(x\) to its integer representation \(x_q\), we solve:</p>
<p>$$x_q = \text{round}\left(\frac{x}{S} + Z\right).$$</p>
<p>Values outside the representable range of the target type
(e.g., [-128, 127] for int8) are typically clipped to the nearest representable
value. Symmetric quantization is a simplified version where the zero-point (Z)
is forced to be 0. This is achieved by choosing a symmetric range around zero
for quantization (e.g., [-max_abs_value, +max_abs_value]).</p>
<h2 id="3-types-of-quantization--calibration"><a class="header" href="#3-types-of-quantization--calibration">3. Types of Quantization &amp; Calibration</a></h2>
<p>To quantize a model, we need not only quantize the tensors holding the weights,
but also the tensors holding the activations — otherwise the computation mix
data types. There are different quantization approaches, each with its
trade-offs in terms of performance, and implementation complexity.
In all cases, we need to determine the range of values for the weights
and activations. This is known as <em>calibration</em>. Calibrating weights is
straightforward, because they are static, but calibrating activations is more
challenging because they are data dependent:</p>
<ul>
<li>
<p><strong>Dynamic Quantization</strong></p>
<ul>
<li><strong>How it works:</strong> Weights are quantized when loading the model. Activations
are quantized <em>dynamically</em> just before compute operations and de-quantized back
to high-precision afterwards.</li>
<li><strong>Pros:</strong> Easiest to implement and reduces model weight memory footprint.</li>
<li><strong>Cons:</strong> Activations are still stored and passed between layers in
high-precision format, limiting memory bandwidth and throughput benefits.</li>
</ul>
</li>
<li>
<p><strong>Static Quantization</strong></p>
<ul>
<li><strong>How it works:</strong> Both weights and activations are quantized. <em>Static</em>
quantization requires a calibration step to determine the ranges for activation
tensors. This is done by running a representative dataset through the
high-precision model to collect activation statistics and compute the activation
ranges. These statistics are then used to calculate the (S, Z) parameters
<em>before</em> run-time.</li>
<li><strong>Pros:</strong> Better throughput than dynamic quantization as both weights and
activations are quantized.</li>
<li><strong>Cons:</strong> Requires collection of a calibration dataset and offline
calibration step.</li>
</ul>
</li>
</ul>
<h2 id="4-limitations-and-considerations"><a class="header" href="#4-limitations-and-considerations">4. Limitations and Considerations</a></h2>
<ul>
<li><strong>Performance Degradation:</strong> Quantization, especially to very low-precision
datatypes, can lead to some loss of performance. Careful evaluation is needed
for peformance-sensitive applications.</li>
<li><strong>Hardware and Operator Support:</strong> Quantization support is not universal.
It depends on the target hardware and the deep learning framework. Not all operators
might have efficient quantized implementations on all platforms. Make sure to verify
your framework and hardware documentation for compatibility.</li>
</ul>
<h4 id="references--useful-links-4"><a class="header" href="#references--useful-links-4">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/abs/1712.05877">Jacob, Benoit, et al. "Quantization and training of
neural networks for efficient integer-arithmetic-only inference", CVPR 2018</a></li>
<li><a href="https://pytorch.org/docs/stable/quantization.html">PyTorch Quantization Documentation</a></li>
</ol>
<!-- markdownlint-disable-file MD033 -->
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/jwilles">
<img src="https://github.com/jwilles.png"
  width="32px" alt="Contributor jwilles" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="kv-cache"><a class="header" href="#kv-cache">KV Cache</a></h1>
<!-- Header -->
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 5 min</small></p>
  </div>
</div>
<!-- Main Body -->
<p>With autoregressive models like decoder-only LLMs (i.e., GPTs), inference is performed
by predicting one token at a time, using the past token generations as inputs for
future ones. To predict future tokens, certain computed representations of these
past tokens are required for every future token prediction. This makes it computationally
inefficient to recalculate these representations at each token generation step."</p>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/nlp/kv-cache-final.excalidraw.svg" alt="kv-cache"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: KV Cache for autoregressive inference
</div>
<p>To formalize this, let \(x_1,x_2, \ldots, x_{t-1}\) represent the input sequence
of \(h\) dimensional embeddings i.e., \(x_i \in R^{1\times h}\). For simplicity,
let's consider a single <a href="llms/efficient_inference/../architecture/attention.html">Attention</a> head and a single
<a href="llms/efficient_inference/../architecture/transformer.html">Transformer</a> block. In order to get the logits
for the next token, the LLM must compute the contextualized vector \(c_{t-1}\)
given by:</p>
<p>$$
\begin{aligned}
c_{t-1} &amp;= f_{attn}(x_1, x_2, \ldots, x_{t-1}; W_k, W_v, W_q),
\end{aligned}
$$</p>
<p>where \(f_{attn}(\cdot)\) is the attention operator that produces a contextualized
vector using all of the input embedding vectors, and \(W_k\), \(W_v\) and \(W_q\)
are the \(h\times h\) projection matrices for keys, values, and queries, respectively.
(Note that the Attention module computes the contextualized vectors of all input
embeddings simultaneously, employing causal masking to ensure that each token
only attends to itself and previous tokens in the sequence.)</p>
<p>Recall that with the attention operator, we first need to compute the various keys
and values representations of the input embeddings as well as the query
representation of \(x_{t-1}\):</p>
<p>$$
K_{t-1} = \begin{bmatrix}
x_1 W_k \\
x_2 W_k \\
\vdots \\
x_{t-1} W_k
\end{bmatrix},
\quad
V_{t-1} = \begin{bmatrix}
x_1 W_v \\
x_2 W_v \\
\vdots \\
x_{t-1} W_v
\end{bmatrix},
\quad
\text{and}
\quad
q_{t-1} = x_{t-1}W_q.
$$</p>
<p>Using scaled-dot attention, we combine the keys with the query to derive an attention
weights vector via:</p>
<p>$$
a_{t-1} = \text{Softmax}(q_{t-1} K_{t-1}^T / \sqrt{h}).
$$</p>
<p>Finally, the contextualized vector of the (\(t-1)\)-th token is the attention-weighted
combination of the values vectors:</p>
<p>$$
c_{t-1} = a_{t-1} V_{t-1}.
$$</p>
<p>The LLM ultimately makes use of this contexualized vector to build the logits for
the \(t\)-th token prediction. Let's suppose that \(x_t\) is generated from
such logits.</p>
<p>With \(x_t\) generated, we aim to predict the next token. To do so, we now
need to build the contextualized vector, \(c_t\):</p>
<p>$$
\begin{aligned}
c_t &amp;= f_{attn}(x_1, x_2, \ldots, x_{t-1}, x_t; W_k, W_v, W_q),
\end{aligned}
$$</p>
<p>As before, we understand that in order to apply this operator, the following keys,
values and query are required:</p>
<p>$$
K_{t} = \begin{bmatrix}
x_1 W_k \\
x_2 W_k \\
\vdots \\
x_{t-1} W_k \\
x_t W_k
\end{bmatrix},
\quad
V_{t} = \begin{bmatrix}
x_1 W_v \\
x_2 W_v \\
\vdots \\
x_{t-1} W_v \\
x_t W_v
\end{bmatrix},
\quad
\text{and}
\quad
q_t = x_{t}W_q.
$$</p>
<p>It immediately follows though that</p>
<p>$$
K_{t} = \begin{bmatrix}
K_{t-1} \\
x_t W_k
\end{bmatrix}
\quad
\text{and}
\quad
V_{t} = \begin{bmatrix}
V_{t-1} \\
x_t W_v
\end{bmatrix}.
$$</p>
<p>In other words, the keys and values required to build \(c_t\) consist of all the
previous keys and values needed for \(c_{t-1}\) plus only the new key and value
derived from the latest input embedding token \(x_t\).</p>
<p>This insight presents an opportunity to significantly reduce computational overhead
during generation by caching and reusing past keys and values rather than recomputing
them.</p>
<p>This is exactly the purpose of having a KV Cache. At each iteration of inference,
we compute the newest key and value emanating from the latest input embedding
token and add it to the respective caches, one for each keys and values.</p>
<blockquote>
<p><strong>Algorithm: KV Cache for Autoregressive Inference</strong></p>
<p><em>Pre-fill Stage</em><br />
Given input sequence \(x_1, x_2, \ldots, x_n\)<br />
Initialize key cache \(K_n = [x_1W_k; x_2W_k; \ldots; x_nW_k]\)<br />
Initialize value cache \(V_n = [x_1W_v; x_2W_v; \ldots; x_nW_v]\)</p>
<p><em>Decode Stage</em><br />
Loop for each token generation step t &gt; n:<br />
\(\quad\)Compute new key and value: \(k_t = x_t W_k\), \(v_t = x_t W_v\)<br />
\(\quad\)Update caches by appending new key and value:<br />
\(\qquad\)\(K_t = [K_{t-1}; k_t]\)<br />
\(\qquad\)\(V_t = [V_{t-1}; v_t]\)<br />
\(\quad\)Compute attention using cached keys and values:<br />
\(\qquad\)\(q_t = x_t W_q\)<br />
\(\qquad\)\(c_t = \text{Softmax}(q_t K_t^T / \sqrt{h}) V_t\)<br />
\(\quad\)Compute next token logits using \(c_t\)<br />
\(\quad\)Generate \(x_{t+1}\) // (which becomes part of the next step's input)</p>
</blockquote>
<h2 id="limitations-4"><a class="header" href="#limitations-4">Limitations</a></h2>
<p>Note that LLMs use Multi-Head Attention modules with several Transformer layers.
Each attention head would need to maintain its own KV Cache and as such, the memory
requirements can be quite expensive. As one example, Liu et al (2024) note that
with 540B PaLM, using a batch size of 512 and context length of 2048, would required
a KV Cache that can take upwards of 3TB of memory — more than the amount of memory
required to hold the model's weight parameters.</p>
<p>This memory bottleneck becomes especially pronounced when serving multiple requests
simultaneously or working with very long context windows.</p>
<h4 id="references--useful-links-5"><a class="header" href="#references--useful-links-5">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2402.02750">Liu, Zirui, et al. "Kivi: A tuning-free asymmetric 2bit quantization for kv
cache." arXiv preprint arXiv:2402.02750 (2024).</a></li>
<li><a href="https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167"><em>Raschka, Sebastian. Build a Large Language Model (From Scratch). Simon and
Schuster, 2024.</em></a></li>
<li><a href="https://r4j4n.github.io/blogs/posts/kv/">Rajan, R. "KV Cache - Understanding the Mechanism behind it." R4J4N Blogs,
r4j4n.github.io/blogs/posts/kv/. Accessed 27 Feb. 2025.</a></li>
</ol>
<!-- Contributions -->
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="notable-models"><a class="header" href="#notable-models">Notable Models</a></h1>
<p>In this section, we provide Pocket References to a few of the storied and important
LLMs to date. Examples include DeepSeek's R1 and V3 models as well as models from
Meta, Google, Alibaba etc.</p>
<p>If you'd like to see another model covered in this section, please do submit a
<a href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new/choose">New Pocket Reference Issue</a>
to our Github project.</p>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="deepseek-r1"><a class="header" href="#deepseek-r1">DeepSeek-R1</a></h1>
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 7 min</small></p>
  </div>
</div>
<p>The DeepSeek-R1 model was introduced by DeepSeek in January of 2025. It is
derived from an earlier checkpoint of <a href="models/../models/deepseek_v3.html">DeepSeek-V3</a>.
In particular, starting with DeepSeek-V3-base, four stages of fine-tuning were
performed in order to arrive at the checkpoint known as DeepSeek-R1: (i) <strong>Reasoning
Cold-Start</strong> (using <a href="models/../llms/fine_tuning/sft.html">SFT</a>), (ii) <strong>RL for Reasoning</strong>
(using <a href="models/../llms/fine_tuning/grpo.html">GRPO</a>), (iii) <strong>SFT for Enhanced Reasoning
&amp; General Capabilities</strong> (using RL-generated reasoning data sampled with
<a href="models/../llms/misc/rejection_sampling.html">Rejection Sampling</a>), and (iv) <strong>RL for Alignment</strong>
(to human preferences).</p>
<p><img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/deepseek-v3-r1-lineage.excalidraw.svg" alt="Lineage" /></p>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
<p>Figure: Illustrating DeepSeek-R1 model evolution.</p>
</div>
<p>As illustrated in the Figure above, the model lineage of DeepSeek-R1 implements
a full-scale RL for reasoning stage that leverages cold-start data. In contrast,
DeepSeek-R1-Zero does not use any cold-start SFT data whatsoever and uses purely
RL steps to acquire its reasoning capabilities. The reward signal used for
guiding the RL process of DeepSeek-R1-Zero is rules based computed from the
response's correctness as well as its adherence to the desired format. While
DeepSeek-R1-Zero demonstrated remarkable reasoning capabilities, it suffered greatly
from poor readability and language mixing.</p>
<p>This motivated the usage of cold-start data in the RL for Reasoning stage of
DeepSeek-R1's training. Additionally, a reward signal to reduce language mixing
as well as a model-based reward (using DeepSeek-V3 for judgement) was also
incorporated.</p>
<h2 id="historical-significance"><a class="header" href="#historical-significance">Historical Significance</a></h2>
<p>At the time of its release, LLM reasoning models such as the OpenAI's o-series
models had demonstrated remarkable performance on complex tasks, including those
requiring multiple steps (e.g., <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">OpenAI o3's breakthrough score on ARC-AGI</a>).
However, OpenAI—operating under a closed-source model—provided little details to
how these models were developed, merely mentioning that Reinforcement Learning techniques
were used to train the LLMs to produce long (internal) chain-of-thought style
reasoning prior to providing a final answer.</p>
<p>In contrast, DeepSeek open-sourced DeepSeek-R1 and provided a very detailed
technical report, shedding much light on its training pipeline, which included an
RL approach for the model to acquire its reasoning capabilities. It was also
reported that DeepSeek-R1 was trained on NVIDIA H800's, a less capable GPU than
the NVIDIA H100 or A100.</p>
<blockquote>
<p>DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs.</p>
<p><em>(quoted from the DeepSeek-V3 Technical Report)</em></p>
</blockquote>
<p>The fact that DeepSeek-R1's performance rivaled that of it's closed-source
counterpart in OpenAI o3 on multiple benchmarks (using reportedly less compute)
led to a frenzy in the LLM and broader AI community. As an example, many teams
(including at least one from HuggingFace) worked tirelessly to produce their own
versions of DeepSeek-R1 in the days after its release.</p>
<h2 id="architectural-highlights"><a class="header" href="#architectural-highlights">Architectural Highlights</a></h2>
<p>See <a href="models/../models/deepseek_v3.html">DeepSeek-V3</a>.</p>
<h2 id="training-data"><a class="header" href="#training-data">Training Data</a></h2>
<p>The training data used for the four stages are described below:</p>
<p><strong>Reasoning Cold Start</strong>: 1000s of samples of long CoT passages from multiple domains,
verified by human annotators was used.</p>
<p><strong>RL for Reasoning</strong>: self-exploration, using increased test-time for RL discovery
until convergence (referred to as the RL checkpoint).</p>
<p><strong>SFT for Enhanced Reasoning &amp; General Capabilities</strong>: the RL checkpoint was then
used to generate 600K reasoning related samples (using rejection sampling).
DeepSeek-V3 was used to create 200K non-reasoning data omitting the CoT portion
for simple queries.</p>
<p><strong>RL for Alignment</strong>: a combination of reward signals diverse data distributions
including preference pairs and analyses of generated summaries &amp; responses.</p>
<h2 id="key-results"><a class="header" href="#key-results">Key Results</a></h2>
<p>Below are three key results of DeepSeek-R1 and its development:</p>
<!-- markdownlint-disable MD013 -->
<div class="table-wrapper"><table><thead><tr><th>Benchmark (Metric)</th><th>Claude-3.5-Sonnet-1022</th><th>GPT-4 0513</th><th>DeepSeek-V3</th><th>OpenAI o1-mini</th><th>OpenAI o1-1217</th><th>DeepSeek-R1</th></tr></thead><tbody>
<tr><td>MMLU (Pass@1)</td><td>88.3</td><td>87.2</td><td>88.5</td><td>85.2</td><td><strong>91.8</strong></td><td>90.8</td></tr>
<tr><td>MMLU-Redux (EM)</td><td>88.9</td><td>88.0</td><td>89.1</td><td>86.7</td><td>-</td><td><strong>92.9</strong></td></tr>
<tr><td>MMLU-Pro (EM)</td><td>78.0</td><td>72.6</td><td>75.9</td><td>80.3</td><td>-</td><td><strong>84.0</strong></td></tr>
<tr><td>DROP (3-shot F1)</td><td>88.3</td><td>83.7</td><td>91.6</td><td>83.9</td><td>90.2</td><td><strong>92.2</strong></td></tr>
<tr><td>IF-Eval (Prompt Strict)</td><td><strong>86.5</strong></td><td>84.3</td><td>86.1</td><td>84.8</td><td>-</td><td>83.3</td></tr>
<tr><td>GFQA Diamond (Pass@1)</td><td>65.0</td><td>49.9</td><td>59.1</td><td>60.0</td><td><strong>75.7</strong></td><td>71.5</td></tr>
<tr><td>SimpleQA (Correct)</td><td>28.4</td><td>38.2</td><td>24.9</td><td>7.0</td><td><strong>47.0</strong></td><td>30.1</td></tr>
<tr><td>FRAMES (Acc.)</td><td>72.5</td><td>80.5</td><td>73.3</td><td>76.9</td><td>-</td><td><strong>82.5</strong></td></tr>
<tr><td>AlpacaEval2.0 (LC-winrate)</td><td>52.0</td><td>51.1</td><td>70.0</td><td>57.8</td><td>-</td><td><strong>87.6</strong></td></tr>
<tr><td>ArenaHard (GPT-4-1106)</td><td>85.2</td><td>80.4</td><td>85.5</td><td>92.0</td><td>-</td><td><strong>92.3</strong></td></tr>
<tr><td>LiveCodeBench (Pass@1-COT)</td><td>38.9</td><td>32.9</td><td>36.2</td><td>53.8</td><td>63.4</td><td><strong>65.9</strong></td></tr>
<tr><td>Codeforces (Percentile)</td><td>20.3</td><td>23.6</td><td>58.7</td><td>93.4</td><td><strong>96.6</strong></td><td>96.3</td></tr>
<tr><td>Codeforces (Rating)</td><td>717</td><td>759</td><td>1134</td><td>1820</td><td><strong>2061</strong></td><td>2029</td></tr>
<tr><td>SWE Verified (Resolved)</td><td><strong>50.8</strong></td><td>38.8</td><td>42.0</td><td>41.6</td><td>48.9</td><td>49.2</td></tr>
<tr><td>Aider-Polyglot (Acc.)</td><td>45.3</td><td>16.0</td><td>49.6</td><td>32.9</td><td><strong>61.7</strong></td><td>53.3</td></tr>
<tr><td>AIME 2024 (Pass@1)</td><td>16.0</td><td>9.3</td><td>39.2</td><td>63.6</td><td>79.2</td><td><strong>79.8</strong></td></tr>
<tr><td>MATH-500 (Pass@1)</td><td>78.3</td><td>74.6</td><td>90.2</td><td>90.0</td><td>96.4</td><td><strong>97.3</strong></td></tr>
<tr><td>CNMO 2024 (Pass@1)</td><td>13.1</td><td>10.8</td><td>43.2</td><td>67.6</td><td>-</td><td><strong>78.8</strong></td></tr>
<tr><td>CLUEWSC (EM)</td><td>85.4</td><td>87.9</td><td>90.9</td><td>89.9</td><td>-</td><td><strong>92.8</strong></td></tr>
<tr><td>C-Eval (EM)</td><td>76.7</td><td>76.0</td><td>86.5</td><td>68.9</td><td>-</td><td><strong>91.8</strong></td></tr>
<tr><td>C-SimpleQA (Correct)</td><td>55.4</td><td>58.7</td><td><strong>68.0</strong></td><td>40.3</td><td>-</td><td>63.7</td></tr>
</tbody></table>
</div><!-- markdownlint-enable MD013 -->
<div
  class="table-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
<p>Table: Comparison between DeepSeek-R1 and other representative models.
(Copied from Table 4 of Guo, Daya, et al (2025).)</p>
</div>
<ol>
<li>
<p><strong>Performance on Benchmarks:</strong> The table above which was copied from the DeepSeek-R1
paper compares the performance of DeepSeek-R1 and -V3 with representative models
from Anthropic and OpenAI. The values reported clearly demonstrate the impressive
performance of DeepSeek-R1 across various benchmarks and tasks. Most notably,
DeepSeek-R1 was able to surpass OpenAI's reasoning model o1-1217 on several benchmarks.</p>
</li>
<li>
<p><strong>Distilling Reasoning Capabilities:</strong> The 800K samples that included generated
examples by both DeepSeek-R1 (reasoning) and DeepSeek-V3 (non-reasoning) were
used to distill other open-source models like <a href="models/../models/qwen2pt5.html">Qwen</a>
and <a href="models/../models/llama_3.html">Llama</a>. With only the application SFT (i.e., no RL),
some of these distilled models were not only able to outperform OpenAI's non-reasoning
model GPT-4o-0513 across all benchmarks tested, but also OpenAI's o1-mini model
on most benchmarks.</p>
</li>
<li>
<p><strong>RL's Potential:</strong> Pure RL empowered DeepSeek-R1-Zero to autonomously acquire
robust reasoning capabilities without any SFT data. What's more is that as test-time
computation was increased, desirable behaviours such as reflection and re-evaluation
on past trajectories emerged making it possible for the model to have "aha moments"
when solving complex tasks. This development should serve as a reminder of the
great potential of RL and its overall place in AI as endeavour to reach new
heights.</p>
</li>
</ol>
<h2 id="limitations-5"><a class="header" href="#limitations-5">Limitations</a></h2>
<p>DeepSeek reported various limitations for DeepSeek-R1. Most notably, DeepSeek-R1
is inferior to DeepSeek-V3 in general capabilities such as function calling, producing
structured outputs (JSON), role-playing, and multi-turn conversations. Additionally,
due to its optimization for English and Chinese, the model sometimes suffers from
language mixing. Lastly, DeepSeek-R1 reportedly demonstrated a high sensitivity
to prompts and long inference times, making it unsuitable for low-latency applications
such as software-engineering tasks.</p>
<h4 id="references--useful-links-6"><a class="header" href="#references--useful-links-6">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2501.12948"><em>Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms
via reinforcement learning." arXiv preprint arXiv:2501.12948 (2025).</em></a></li>
<li><a href="https://arxiv.org/pdf/2412.19437"><em>Liu, Aixin, et al. "Deepseek-v3 technical report." arXiv preprint
arXiv:2412.19437 (2024).</em></a></li>
<li><a href="https://fortune.com/2025/01/27/china-deepseek-nvidia-gpu-investor-panic-us-export-controls-rethink/"><em>China's DeepSeek sets off Nvidia investor panic over US export controls</em></a>
<em>(appearing in fortune.com)</em></li>
<li><a href="https://huggingface.co/blog/open-r1"><em>Open-R1: a fully open reproduction of DeepSeek-R1</em></a>
<em>(by HuggingFace)</em></li>
<li><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"><em>DeepSeek-R1 is available on HuggingFace</em></a></li>
</ol>
<!-- markdownlint-disable-file MD033 -->
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="deepseek-v3"><a class="header" href="#deepseek-v3">DeepSeek-v3</a></h1>
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 7 min</small></p>
  </div>
</div>
<p>The DeepSeek-V3 model was introduced by DeepSeek in December of 2024. It is an
LLM that leverages <a href="models/../llms/architecture/moe.html">MoE</a> in its design.</p>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/deepseek-v3-lineage-v2.excalidraw.svg" alt="DeepSeek-V3 Model Lineage"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: Illustrating DeepSeek-V3 training evolution.
</div>
<p>The training pipeline for DeepSeek-V3 consists of the two typical stages: pre-training
and post-training. As depicted in the Figure above, the pre-training stage involves
pre-training on 14.8T tokens followed by long-context extension using the <a href="models/../llms/fine_tuning/yarn.html">YaRN</a>
methodology. Post-training of DeepSeek-V3 utilizes <a href="models/../llms/fine_tuning/sft.html">SFT</a>
as well as Reinforcement Learning methods.</p>
<h2 id="historical-significance-1"><a class="header" href="#historical-significance-1">Historical Significance</a></h2>
<p>At the time of its release, open-source models had already been lessening the gap
in performance with closed-source counterparts. DeepSeek-V3 was yet another open-source
model that achieved high levels of performance, beating other open-source alternatives
as well as some closed-source models in various benchmarks. What made DeepSeek-V3's
achievement even more intriguing was that it was reportedly trained using less
compute than its closest counterparts.</p>
<h2 id="architectural-highlights-1"><a class="header" href="#architectural-highlights-1">Architectural Highlights</a></h2>
<p>DeepSeek-V3 is a transformer-based model that swaps out nearly all dense <a href="models/../llms/architecture/feedforward.html">feedforward</a>
for <a href="models/../llms/architecture/moe.html">MoE</a>. The model has a total of 671B parameters
but through its specialized variant of MoE (referred to as DeepSeekMoE), only
37B parameters are activated in both training and inference. Through a series of
long-context extension fine-tuning steps, the maximum context length for this model
was extended to 128K tokens.</p>
<p><strong>DeepSeekMoE:</strong> Used to carry out training more efficiently, this MoE design
consists of two sets of experts, namely: shared and routed. The former set of routers
is used for every token in the input sequence whereas the usage of routed ones are
determined according to the affinity to the input token.</p>
<p><strong>Auxiliary-loss Load Free Balancing:</strong> When using an MoE architecture, one must
consider load balancing across the networks to prevent routing collapse. This has
been typically addressed via the introduction of an auxiliary loss. However, if
this loss has too great of an influence, it can lead to a model degradation. DeepSeek-V3
instead considers a technique that requires no auxiliary loss but instead relies
on a new bias term that dynamically changes its value according to the experts
current workload.</p>
<p><strong>Multi-Head Latent Attention (MLA):</strong> Used for making inference more efficient
by jointly compressing attention keys and values to a lower dimension. The compression
involves a linear projection matrix compressing keys and values down as well as
another linear project matrix for compressing keys and values back up. Only the
compressed joint representation of keys and values need to be cached during inference.
For more details see <a href="models/../llms/architecture/mla.html">MLA</a>.</p>
<p><strong>Multi-Token Prediction:</strong> In an effort to improve the training signal, DeepSeek-V3
expands the prediction scope to additional future tokens at every token position
of the sequence. In other words, instead of only predicting the next immediate token
and training the model on this signal, $D$ future tokens are predicted. These tokens
are predicted sequentially by $D$ sequential multi-token prediction modules in order
to maintain the causal chain. For more details see <a href="models/../llms/decoding/multi_token_prediction.html">MTP</a>.</p>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Value</th></tr></thead><tbody>
<tr><td>Total parameters</td><td>671B</td></tr>
<tr><td>Activated parameters</td><td>37B</td></tr>
<tr><td>Maximum context length</td><td>128K tokens</td></tr>
<tr><td>Number of Transformer layers</td><td>61</td></tr>
<tr><td>Hidden dimension size</td><td>7168</td></tr>
<tr><td>Number of attention heads</td><td>128</td></tr>
<tr><td>Number of experts (MoE)</td><td>1 (shared) &amp; 256 (routed)</td></tr>
<tr><td>Hidden dimension of experts</td><td>2048</td></tr>
<tr><td>KV compression dimension size (MLA)</td><td>512</td></tr>
<tr><td>Multi-token depth (MTP)</td><td>1</td></tr>
</tbody></table>
</div><div
  class="table-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Table 1: Summary of DeepSeek-V3 architecture and hyper parameters.
</div>
<h2 id="training-data-1"><a class="header" href="#training-data-1">Training Data</a></h2>
<p>The pre-training corpus is a revised version of the one used to train an earlier
version of the model, DeepSeek-V2. In this revision, more samples pertaining to
mathematics and programming were included. Ultimately, the dataset comprised of
14.8T tokens.</p>
<h2 id="compute-details"><a class="header" href="#compute-details">Compute Details</a></h2>
<p>DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs. Each node within
the cluster consists of 8 H800 GPUs inter-connected via NVLink and NVSwitch. In total,
it was reported that only 2.664M H800 GPU hours were used for pre-training while
subsequent training stages required only 0.1M GPU hours. One of the main reasons
for this training efficiency was their application of an FP8 mixed precision
training framework.</p>
<h2 id="key-results-1"><a class="header" href="#key-results-1">Key Results</a></h2>
<!-- markdownlint-disable MD013 -->
<div class="table-wrapper"><table><thead><tr><th>Benchmark (Metric)</th><th># Shots</th><th>DeepSeek-V2 Base</th><th>Qwen2.5 72B Base</th><th>LLaMA-3.1 405B Base</th><th>DeepSeek-V3 Base</th></tr></thead><tbody>
<tr><td>Pile-test (BPB)</td><td>-</td><td>0.606</td><td>0.638</td><td><strong>0.542</strong></td><td>0.548</td></tr>
<tr><td>BBH (EM)</td><td>3-shot</td><td>78.8</td><td>79.8</td><td>82.9</td><td><strong>87.5</strong></td></tr>
<tr><td>MMLU (EM)</td><td>5-shot</td><td>78.4</td><td>85.0</td><td>84.4</td><td><strong>87.1</strong></td></tr>
<tr><td>MMLU-Redux (EM)</td><td>5-shot</td><td>75.6</td><td>83.2</td><td>81.3</td><td><strong>86.2</strong></td></tr>
<tr><td>MMLU-Pro (EM)</td><td>5-shot</td><td>51.4</td><td>58.3</td><td>52.8</td><td><strong>64.4</strong></td></tr>
<tr><td>DROP (F1)</td><td>3-shot</td><td>80.4</td><td>80.6</td><td>86.0</td><td><strong>89.0</strong></td></tr>
<tr><td>ARC-Easy (EM)</td><td>25-shot</td><td>97.6</td><td>98.4</td><td>98.4</td><td><strong>98.9</strong></td></tr>
<tr><td>ARC-Challenge (EM)</td><td>25-shot</td><td>92.2</td><td>94.5</td><td><strong>95.3</strong></td><td><strong>95.3</strong></td></tr>
<tr><td>HellaSwag (EM)</td><td>10-shot</td><td>87.1</td><td>84.8</td><td><strong>89.2</strong></td><td>88.9</td></tr>
<tr><td>PIQA (EM)</td><td>0-shot</td><td>83.9</td><td>82.1</td><td><strong>85.9</strong></td><td>84.7</td></tr>
<tr><td>WinoGrande (EM)</td><td>5-shot</td><td><strong>86.3</strong></td><td>82.3</td><td>85.2</td><td>84.9</td></tr>
<tr><td>RACE-Middle (EM)</td><td>3-shot</td><td>73.1</td><td>68.1</td><td><strong>74.2</strong></td><td>74.9</td></tr>
<tr><td>RACE-High (EM)</td><td>5-shot</td><td>52.6</td><td>50.3</td><td><strong>56.8</strong></td><td>51.3</td></tr>
<tr><td>TriviaQA (EM)</td><td>5-shot</td><td>80.0</td><td>71.9</td><td><strong>82.7</strong></td><td>82.9</td></tr>
<tr><td>NaturalQuestions (EM)</td><td>5-shot</td><td>38.6</td><td>33.2</td><td><strong>41.5</strong></td><td>40.0</td></tr>
<tr><td>AGIEval (EM)</td><td>0-shot</td><td>57.5</td><td>75.8</td><td>60.6</td><td><strong>79.6</strong></td></tr>
<tr><td>HumanEval (Pass@1)</td><td>0-shot</td><td>43.3</td><td>53.0</td><td>54.9</td><td><strong>65.2</strong></td></tr>
<tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>65.0</td><td>72.6</td><td>68.4</td><td><strong>75.4</strong></td></tr>
<tr><td>LiveCodeBench-Base (Pass@1)</td><td>3-shot</td><td>11.6</td><td>12.9</td><td>15.1</td><td><strong>19.4</strong></td></tr>
<tr><td>CRUXEval-1 (EM)</td><td>2-shot</td><td>52.5</td><td>59.1</td><td>58.5</td><td><strong>67.3</strong></td></tr>
<tr><td>CRUXEval-O (EM)</td><td>2-shot</td><td>49.8</td><td>59.9</td><td>59.9</td><td><strong>69.8</strong></td></tr>
<tr><td>CSMRR (EM)</td><td>8-shot</td><td>81.6</td><td>88.3</td><td>89.3</td><td><strong>89.3</strong></td></tr>
<tr><td>MATH (EM)</td><td>4-shot</td><td>43.4</td><td>54.4</td><td>49.0</td><td><strong>61.6</strong></td></tr>
<tr><td>MGSM (EM)</td><td>8-shot</td><td>63.6</td><td>76.2</td><td>69.9</td><td><strong>79.8</strong></td></tr>
<tr><td>CMath (EM)</td><td>3-shot</td><td>78.7</td><td>84.5</td><td>77.3</td><td><strong>90.7</strong></td></tr>
<tr><td>CLUEWSC (EM)</td><td>5-shot</td><td>82.0</td><td>82.5</td><td><strong>83.0</strong></td><td>82.7</td></tr>
<tr><td>C-Eval (EM)</td><td>0-shot</td><td>81.4</td><td>72.5</td><td>72.5</td><td><strong>90.1</strong></td></tr>
<tr><td>CMMLU (EM)</td><td>5-shot</td><td>84.0</td><td><strong>89.5</strong></td><td>73.7</td><td>88.8</td></tr>
<tr><td>CMRC (EM)</td><td>1-shot</td><td><strong>77.4</strong></td><td>75.8</td><td>76.0</td><td>76.3</td></tr>
<tr><td>C3 (EM)</td><td>0-shot</td><td>77.4</td><td>76.7</td><td><strong>79.7</strong></td><td>78.6</td></tr>
<tr><td>CCPM (EM)</td><td>0-shot</td><td><strong>93.0</strong></td><td>88.5</td><td>78.6</td><td>92.0</td></tr>
<tr><td>MMLU-non-English (EM)</td><td>5-shot</td><td>64.0</td><td>74.8</td><td>73.8</td><td><strong>79.4</strong></td></tr>
</tbody></table>
</div><!-- markdownlint-enable MD013 -->
<div
  class="table-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Table 2: Comparison between DeepSeek-V3 and other representative models.
(Copied from Table 3 of Liu, Aixin, et al (2024).)
</div>
<ol>
<li>
<p><strong>Superior Open-Source Model:</strong> DeepSeek-V3 outperformed all other open-source
models on educational benchmarks (MMLU, MMLU-Pro, GPQA) achieving performance
levels that rivals that for closed-source models such as GPT-4o and Claude-Sonnet-3.5.
DeepSeek-V3 also achieved SOTA on math-related benchmarks (GSM8K, MATH, MGSM,
CMath).</p>
</li>
<li>
<p><strong>Efficient Training:</strong> DeepSeek-V3 was trained using only 2.664M H800 GPU hours,
leveraging an FP8 mixed precision training framework. This marked, as reported
by the authors, the first successful use of an FP8 scheme to train a large-scale
model.</p>
</li>
<li>
<p><strong>Reasoning Distillation:</strong> As part of the post-training step, DeepSeek-V3 creators
were able to distill reasoning capabilities via long <a href="models/../llms/prompting/cot.html">CoT</a>
passages generated by <a href="models/../models/deepseek_r1.html">DeepSeek-R1</a>. The authors noted
that this pipeline improved reasononing performance while still maintaining the
ability to produce desired outputs and efficient response lengths.</p>
</li>
</ol>
<h2 id="limitations-6"><a class="header" href="#limitations-6">Limitations</a></h2>
<p>DeepSeek-V3 requires significant amounts of computation facilities to ensure efficient
inference.</p>
<h4 id="references--useful-links-7"><a class="header" href="#references--useful-links-7">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2412.19437"><em>Liu, Aixin, et al. "Deepseek-v3 technical report." arXiv preprint
arXiv:2412.19437 (2024).</em></a></li>
<li><a href="https://www.reuters.com/technology/chinas-deepseek-sets-off-ai-market-rout-2025-01-27/">DeepSeek sparks AI stock selloff; Nvidia posts record market-cap loss</a>
(<em>appearing in reuters.com</em>)</li>
</ol>
<!-- markdownlint-disable-file MD033 -->
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
