<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>AI Pocket Reference: NLP</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="A streamlined reference manual for AI practitioners, students, and developers to quickly look up core concepts and mock implementations">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../_common/mdbook-ai-pocket-reference.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">AI Pocket Reference: NLP</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://vectorinstitute.github.io/ai-pocket-reference/" title="Home" aria-label="Home">
                            <i id="home-button" class="fa fa-home"></i>
                        </a>
                        <a href="https://github.com/VectorInstitute/ai-pocket-reference" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Welcome to AI Pocket References: NLP Collection. This compilation covers a broad
range of Natural Language Processing topics including foundational LLM concepts,
architectures, prompting techniques, fine-tuning approaches, and evaluation metrics.
These concise references are designed for quick understanding and practical application.</p>
<p>Be sure to check out our other collections of <a href="https://vectorinstitute.github.io/ai-pocket-reference/">AI Pocket References!</a></p>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="chain-of-thought"><a class="header" href="#chain-of-thought">Chain of Thought</a></h1>
<!-- Header -->
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 3 min</small></p>
  </div>
</div>
<!-- Main Body -->
<p>The Chain of Thought (CoT) prompting technique, introduced by Wei, Jason et al (2022),
encourages an LLM to articulate its reasoning steps before arriving at a final
answer to a given task.</p>
<p>Before its introduction, scaling LLMs had demonstrated the ability to generate coherent
text and solve various tasks. However, these LLMs still underperformed on complex
reasoning tasks like arithmetic and symbolic reasoning. While some prompting techniques
and <a href="llms/prompting/./icl.html">in-context learning</a> had already been discovered, none had successfully
enabled LLMs to handle complex reasoning tasks.</p>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/nlp/cot.svg" alt="cot"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: LLM producing a chain of thought.
</div>
<h2 id="original-implementation-details"><a class="header" href="#original-implementation-details">Original Implementation Details</a></h2>
<p>CoT was originally introduced as a few-shot prompting technique where each included
examplar is augmented with a <em>chain of thought</em> that explains how the final answer
was determined. An example of such an examplar taken from the original paper is
provided below:</p>
<pre><code class="language-yaml"># An examplar
examplar:
  question: &gt;
    Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each
    can has 3 tennis balls. How many tennis balls does he have now?
  chain of thought: &gt;
    Roger started with 5 balls. 2 cans of 3 tennis balls each
    is 6 tennis balls. 5 + 6 = 11.
  answer: The answer is 11.
</code></pre>
<p>The authors used the same set of 8 examplars across all tested benchmarks, with
the exception of <a href="https://github.com/google-deepmind/AQuA">AQuA</a> for which 4
examplars derived from the training set was used instead.</p>
<h2 id="performance"><a class="header" href="#performance">Performance</a></h2>
<p>With larger models, CoT outperformed standard prompting across all tested reasoning
benchmarks (mathematical, commonsense, and symbolic). For some of these, it even
achieved state of the art results, beating out previous methods that relied on
fine-tuning. However, CoT added little benefit for smaller models, leading the
authors to posit it as an <a href="llms/prompting/../misc/emergent.html">emergent</a> ability of model
scale.</p>
<h2 id="limitations"><a class="header" href="#limitations">Limitations</a></h2>
<p>One of the noted limitations of CoT is the lack of guarantees on correct reasoning
paths taken by the LLM. In other words, the reasoning steps that the LLM performs
can be flawed, leading to inefficient token generation and potentially amplifying
the issue of LLM hallucinations.</p>
<h2 id="modern-implementations"><a class="header" href="#modern-implementations">Modern Implementations</a></h2>
<p>Since its introdcution, the CoT prompting technique has become more flexible.
Broadly speaking, it is widely recognized as a prompting technique that elicits a
<em>chain of thought</em> output in its generation. To do so, many include general
instructions in the prompt, specifying the desired output format and other requirements.
With these system instructions and output formats, CoT can also be implemented
in a zero-shot fashion.</p>
<pre><code class="language-yaml"># Example CoT prompt instructions
prompt:
  system: &gt;
    You are a helpful assistant that is able to handle complex reasoning
    tasks. To arrive at the final answer, perform chain of thought steps
    and include these in your output.

    Structure your output using the following format
      &lt;thought&gt;
        chain of thought here
      &lt;/thought&gt;
      &lt;answer&gt;
        answer here
      &lt;/answer&gt;
  question: &gt;
    Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each
    can has 3 tennis balls. How many tennis balls does he have now?
</code></pre>
<h4 id="references--useful-links"><a class="header" href="#references--useful-links">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2106.09685"><em>Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large
language models." Advances in neural information processing systems 35 (2022):
24824-24837.</em></a></li>
</ol>
<!-- Contributions -->
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="lora"><a class="header" href="#lora">LoRA</a></h1>
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <a target="_blank" href="https://colab.research.google.com/github/VectorInstitute/ai-pocket-reference-code/blob/main/notebooks/nlp/lora.ipynb">
      <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 3 min</small></p>
  </div>
</div>
<p>Low-rank adaptation (LoRA) is parameter-efficient fine-tuning (<a href="llms/fine_tuning/../fine_tuning/peft.html">PEFT</a>)
introduced by Hu, Edward J. et al. (2021). The creators of LoRA posited that since
trained deep learning models reside in low intrinsic dimensions, perhaps their
weight-update matrices do as well.</p>
<p>Specifically, with LoRA, we learn a low-rank representation of the weight-update
matrices of dense, linear layers of a pre-trained LLM. The original weights
of the LLM are frozen during fine-tuning and only the low-rank weight-update matrices
at each step of fine-tuning. This reduction in dimensionality helps to amplify the
most important or influential features of the model.</p>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/nlp/LoRA.svg" alt="lora"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: Illustrating a forward pass with LoRA
</div>
<h2 id="some-math"><a class="header" href="#some-math">Some Math</a></h2>
<p>Let \(W\) represent the \(d\times d\) weight matrix for a dense, linear layer.
We can then loosely represent an updated version (i.e. after fine-tuning) of
this matrix as follows:</p>
<p>$$W_{\text{updated}} = W + \Delta W,$$</p>
<p>where \(\Delta W\) is the update matrix. With LoRA, it is \(\Delta W\) which
we project into a low-rank space:</p>
<p>$$\Delta W \approx AB,$$</p>
<p>where \(A\) and \(B^T\) are both matrices of dimension \(d \times r\) and
\(r &lt;&lt; d\). During fine-tuning, \(W\) is frozen and only \(A\) and \(B\)
are updated.</p>
<p>For inference (i.e., forward phase), let \(x\) be an input embedding, then by
the distributive property</p>
<p>$$xW_{\text{updated}} = xW + x\Delta W \approx xW + xAB.$$</p>
<h2 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h2>
<p>One modular implementation of LoRA involves the introduction of a <code>LoRALayer</code> that
comprises of only the \(A\) and \(B\) dense weights. In this way, a <code>LoRALayer</code>
can adapt a pre-trained <code>Linear</code> layer.</p>
<pre><code class="language-python">import torch


class LoRALayer(torch.nn.Module):
    """A basic LoRALayer implementation."""

    def __init__(self, d_in: int, d_out: int, rank: int):
        self.A = torch.nn.Parameter(torch.empty(d_in, rank))
        self.B = torch.nn.Parameter(torch.zeros(rank, d_out))

    def forward(self, x):
        return x @ self.A @ self.B
</code></pre>
<p>With the <code>LoRALayer</code> defined in this way, one can then combine this with a <code>Linear</code>
layer to implement the LoRA technique. See the supplementary Colab notebook linked
at the top of this pocket reference for more details.</p>
<h2 id="performance-1"><a class="header" href="#performance-1">Performance</a></h2>
<p>In the original paper, the authors reported similar levels of performance when using
LoRA compared to full fine-tuning on various natural language generation and understanding
tasks.</p>
<h2 id="additional-benefits"><a class="header" href="#additional-benefits">Additional Benefits</a></h2>
<p>Since LoRA matrices can be stored efficiently and separately from the pre-trained
LLM weights, customization of these large models is highly scalable. Organizations
can build libraries of specialized LoRA matrices for different datasets and domains,
switching between them as needed for specific applications.</p>
<h2 id="limitations-1"><a class="header" href="#limitations-1">Limitations</a></h2>
<h4 id="references--useful-links-1"><a class="header" href="#references--useful-links-1">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2106.09685"><em>Hu, Edward J., et al. "Lora: Low-rank adaptation of large language models."
arXiv preprint arXiv:2106.09685, 2021.</em></a></li>
<li><a href="https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167"><em>Raschka, Sebastian. Build a Large Language Model (From Scratch). Simon and
Schuster, 2024.</em></a></li>
<li><a href="https://huggingface.co/docs/peft/en/task_guides/lora_based_methods"><em>Sourab Mangrulkar et al. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods (LoRA methods), 2022.</em></a></li>
<li><a href="https://arxiv.org/pdf/2307.13269"><em>Huang, Chengsong, et al. "Lorahub: Efficient cross-task generalization via
dynamic lora composition." arXiv preprint arXiv:2307.13269 (2023).</em></a></li>
<li><a href="https://d3ddy8balm3goa.cloudfront.net/paper-cards/w29_2023-lora.excalidraw.svg"><em>Fajardo V.A. LoRA PaperCard, 2023.</em></a></li>
</ol>
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="kv-cache"><a class="header" href="#kv-cache">KV Cache</a></h1>
<!-- Header -->
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 5 min</small></p>
  </div>
</div>
<!-- Main Body -->
<p>With autoregressive models like decoder-only LLMs (i.e., GPTs), inference is performed
by predicting one token at a time, using the past token generations as inputs for
future ones. To predict future tokens, certain computed representations of these
past tokens are required for every future token prediction. This makes it computationally
inefficient to recalculate these representations at each token generation step."</p>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/nlp/kv-cache-final.excalidraw.svg" alt="kv-cache"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: KV Cache for autoregressive inference
</div>
<p>To formalize this, let \(x_1,x_2, \ldots, x_{t-1}\) represent the input sequence
of \(h\) dimensional embeddings i.e., \(x_i \in R^{1\times h}\). For simplicity,
let's consider a single <a href="llms/efficient_inference/../architecture/attention.html">Attention</a> head and a single
<a href="llms/efficient_inference/../architecture/transformer.html">Transformer</a> block. In order to get the logits
for the next token, the LLM must compute the contextualized vector \(c_{t-1}\)
given by:</p>
<p>$$
\begin{aligned}
c_{t-1} &amp;= f_{attn}(x_1, x_2, \ldots, x_{t-1}; W_k, W_v, W_q),
\end{aligned}
$$</p>
<p>where \(f_{attn}(\cdot)\) is the attention operator that produces a contextualized
vector using all of the input embedding vectors, and \(W_k\), \(W_v\) and \(W_q\)
are the \(h\times h\) projection matrices for keys, values, and queries, respectively.
(Note that the Attention module computes the contextualized vectors of all input
embeddings simultaneously, employing causal masking to ensure that each token
only attends to itself and previous tokens in the sequence.)</p>
<p>Recall that with the attention operator, we first need to compute the various keys
and values representations of the input embeddings as well as the query
representation of \(x_{t-1}\):</p>
<p>$$
K_{t-1} = \begin{bmatrix}
x_1 W_k \\
x_2 W_k \\
\vdots \\
x_{t-1} W_k
\end{bmatrix},
\quad
V_{t-1} = \begin{bmatrix}
x_1 W_v \\
x_2 W_v \\
\vdots \\
x_{t-1} W_v
\end{bmatrix},
\quad
\text{and}
\quad
q_{t-1} = x_{t-1}W_q.
$$</p>
<p>Using scaled-dot attention, we combine the keys with the query to derive an attention
weights vector via:</p>
<p>$$
a_{t-1} = \text{Softmax}(q_{t-1} K_{t-1}^T / \sqrt{h}).
$$</p>
<p>Finally, the contextualized vector of the (\(t-1)\)-th token is the attention-weighted
combination of the values vectors:</p>
<p>$$
c_{t-1} = a_{t-1} V_{t-1}.
$$</p>
<p>The LLM ultimately makes use of this contexualized vector to build the logits for
the \(t\)-th token prediction. Let's suppose that \(x_t\) is generated from
such logits.</p>
<p>With \(x_t\) generated, we aim to predict the next token. To do so, we now
need to build the contextualized vector, \(c_t\):</p>
<p>$$
\begin{aligned}
c_t &amp;= f_{attn}(x_1, x_2, \ldots, x_{t-1}, x_t; W_k, W_v, W_q),
\end{aligned}
$$</p>
<p>As before, we understand that in order to apply this operator, the following keys,
values and query are required:</p>
<p>$$
K_{t} = \begin{bmatrix}
x_1 W_k \\
x_2 W_k \\
\vdots \\
x_{t-1} W_k \\
x_t W_k
\end{bmatrix},
\quad
V_{t} = \begin{bmatrix}
x_1 W_v \\
x_2 W_v \\
\vdots \\
x_{t-1} W_v \\
x_t W_v
\end{bmatrix},
\quad
\text{and}
\quad
q_t = x_{t}W_q.
$$</p>
<p>It immediately follows though that</p>
<p>$$
K_{t} = \begin{bmatrix}
K_{t-1} \\
x_t W_k
\end{bmatrix}
\quad
\text{and}
\quad
V_{t} = \begin{bmatrix}
V_{t-1} \\
x_t W_v
\end{bmatrix}.
$$</p>
<p>In other words, the keys and values required to build \(c_t\) consist of all the
previous keys and values needed for \(c_{t-1}\) plus only the new key and value
derived from the latest input embedding token \(x_t\).</p>
<p>This insight presents an opportunity to significantly reduce computational overhead
during generation by caching and reusing past keys and values rather than recomputing
them.</p>
<p>This is exactly the purpose of having a KV Cache. At each iteration of inference,
we compute the newest key and value emanating from the latest input embedding
token and add it to the respective caches, one for each keys and values.</p>
<blockquote>
<p><strong>Algorithm: KV Cache for Autoregressive Inference</strong></p>
<p><em>Pre-fill Stage</em><br />
Given input sequence \(x_1, x_2, \ldots, x_n\)<br />
Initialize key cache \(K_n = [x_1W_k; x_2W_k; \ldots; x_nW_k]\)<br />
Initialize value cache \(V_n = [x_1W_v; x_2W_v; \ldots; x_nW_v]\)</p>
<p><em>Decode Stage</em><br />
Loop for each token generation step t &gt; n:<br />
\(\quad\)Compute new key and value: \(k_t = x_t W_k\), \(v_t = x_t W_v\)<br />
\(\quad\)Update caches by appending new key and value:<br />
\(\qquad\)\(K_t = [K_{t-1}; k_t]\)<br />
\(\qquad\)\(V_t = [V_{t-1}; v_t]\)<br />
\(\quad\)Compute attention using cached keys and values:<br />
\(\qquad\)\(q_t = x_t W_q\)<br />
\(\qquad\)\(c_t = \text{Softmax}(q_t K_t^T / \sqrt{h}) V_t\)<br />
\(\quad\)Compute next token logits using \(c_t\)<br />
\(\quad\)Generate \(x_{t+1}\) // (which becomes part of the next step's input)</p>
</blockquote>
<h2 id="limitations-2"><a class="header" href="#limitations-2">Limitations</a></h2>
<p>Note that LLMs use Multi-Head Attention modules with several Transformer layers.
Each attention head would need to maintain its own KV Cache and as such, the memory
requirements can be quite expensive. As one example, Liu et al (2024) note that
with 540B PaLM, using a batch size of 512 and context length of 2048, would required
a KV Cache that can take upwards of 3TB of memory — more than the amount of memory
required to hold the model's weight parameters.</p>
<p>This memory bottleneck becomes especially pronounced when serving multiple requests
simultaneously or working with very long context windows.</p>
<h4 id="references--useful-links-2"><a class="header" href="#references--useful-links-2">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2402.02750">Liu, Zirui, et al. "Kivi: A tuning-free asymmetric 2bit quantization for kv
cache." arXiv preprint arXiv:2402.02750 (2024).</a></li>
<li><a href="https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167"><em>Raschka, Sebastian. Build a Large Language Model (From Scratch). Simon and
Schuster, 2024.</em></a></li>
<li><a href="https://r4j4n.github.io/blogs/posts/kv/">Rajan, R. "KV Cache - Understanding the Mechanism behind it." R4J4N Blogs,
r4j4n.github.io/blogs/posts/kv/. Accessed 27 Feb. 2025.</a></li>
</ol>
<!-- Contributions -->
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="deepseek-r1"><a class="header" href="#deepseek-r1">DeepSeek-R1</a></h1>
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 7 min</small></p>
  </div>
</div>
<p>The DeepSeek-R1 model was introduced by DeepSeek in January of 2025. It is
derived from an earlier checkpoint of <a href="models/../models/deepseek_v3.html">DeepSeek-V3</a>.
In particular, starting with DeepSeek-V3-base, four stages of fine-tuning were
performed in order to arrive at the checkpoint known as DeepSeek-R1: (i) <strong>Reasoning
Cold-Start</strong> (using <a href="models/../llms/fine_tuning/sft.html">SFT</a>), (ii) <strong>RL for Reasoning</strong>
(using <a href="models/../llms/fine_tuning/grpo.html">GRPO</a>), (iii) <strong>SFT for Enhanced Reasoning
&amp; General Capabilities</strong> (using RL-generated reasoning data sampled with
<a href="models/../llms/misc/rejection_sampling.html">Rejection Sampling</a>), and (iv) <strong>RL for Alignment</strong>
(to human preferences).</p>
<p><img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/deepseek-v3-r1-lineage.excalidraw.svg" alt="Lineage" /></p>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
<p>Figure: Illustrating DeepSeek-R1 model evolution.</p>
</div>
<p>As illustrated in the Figure above, the model lineage of DeepSeek-R1 implements
a full-scale RL for reasoning stage that leverages cold-start data. In contrast,
DeepSeek-R1-Zero does not use any cold-start SFT data whatsoever and uses purely
RL steps to acquire its reasoning capabilities. The reward signal used for
guiding the RL process of DeepSeek-R1-Zero is rules based computed from the
response's correctness as well as its adherence to the desired format. While
DeepSeek-R1-Zero demonstrated remarkable reasoning capabilities, it suffered greatly
from poor readability and language mixing.</p>
<p>This motivated the usage of cold-start data in the RL for Reasoning stage of
DeepSeek-R1's training. Additionally, a reward signal to reduce language mixing
as well as a model-based reward (using DeepSeek-V3 for judgement) was also
incorporated.</p>
<h2 id="historical-significance"><a class="header" href="#historical-significance">Historical Significance</a></h2>
<p>At the time of its release, LLM reasoning models such as the OpenAI's o-series
models had demonstrated remarkable performance on complex tasks, including those
requiring multiple steps (e.g., <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">OpenAI o3's breakthrough score on ARC-AGI</a>).
However, OpenAI—operating under a closed-source model—provided little details to
how these models were developed, merely mentioning that Reinforcement Learning techniques
were used to train the LLMs to produce long (internal) chain-of-thought style
reasoning prior to providing a final answer.</p>
<p>In contrast, DeepSeek open-sourced DeepSeek-R1 and provided a very detailed
technical report, shedding much light on its training pipeline, which included an
RL approach for the model to acquire its reasoning capabilities. It was also
reported that DeepSeek-R1 was trained on NVIDIA H800's, a less capable GPU than
the NVIDIA H100 or A100.</p>
<blockquote>
<p>DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs.</p>
<p><em>(quoted from the DeepSeek-V3 Technical Report)</em></p>
</blockquote>
<p>The fact that DeepSeek-R1's performance rivaled that of it's closed-source
counterpart in OpenAI o3 on multiple benchmarks (using reportedly less compute)
led to a frenzy in the LLM and broader AI community. As an example, many teams
(including at least one from HuggingFace) worked tirelessly to produce their own
versions of DeepSeek-R1 in the days after its release.</p>
<h2 id="architectural-highlights"><a class="header" href="#architectural-highlights">Architectural Highlights</a></h2>
<p>See <a href="models/../models/deepseek_v3.html">DeepSeek-V3</a>.</p>
<h2 id="training-data"><a class="header" href="#training-data">Training Data</a></h2>
<p>The training data used for the four stages are described below:</p>
<p><strong>Reasoning Cold Start</strong>: 1000s of samples of long CoT passages from multiple domains,
verified by human annotators was used.</p>
<p><strong>RL for Reasoning</strong>: self-exploration, using increased test-time for RL discovery
until convergence (referred to as the RL checkpoint).</p>
<p><strong>SFT for Enhanced Reasoning &amp; General Capabilities</strong>: the RL checkpoint was then
used to generate 600K reasoning related samples (using rejection sampling).
DeepSeek-V3 was used to create 200K non-reasoning data omitting the CoT portion
for simple queries.</p>
<p><strong>RL for Alignment</strong>: a combination of reward signals diverse data distributions
including preference pairs and analyses of generated summaries &amp; responses.</p>
<h2 id="key-results"><a class="header" href="#key-results">Key Results</a></h2>
<p>Below are three key results of DeepSeek-R1 and its development:</p>
<!-- markdownlint-disable MD013 -->
<div class="table-wrapper"><table><thead><tr><th>Benchmark (Metric)</th><th>Claude-3.5-Sonnet-1022</th><th>GPT-4 0513</th><th>DeepSeek-V3</th><th>OpenAI o1-mini</th><th>OpenAI o1-1217</th><th>DeepSeek-R1</th></tr></thead><tbody>
<tr><td>MMLU (Pass@1)</td><td>88.3</td><td>87.2</td><td>88.5</td><td>85.2</td><td><strong>91.8</strong></td><td>90.8</td></tr>
<tr><td>MMLU-Redux (EM)</td><td>88.9</td><td>88.0</td><td>89.1</td><td>86.7</td><td>-</td><td><strong>92.9</strong></td></tr>
<tr><td>MMLU-Pro (EM)</td><td>78.0</td><td>72.6</td><td>75.9</td><td>80.3</td><td>-</td><td><strong>84.0</strong></td></tr>
<tr><td>DROP (3-shot F1)</td><td>88.3</td><td>83.7</td><td>91.6</td><td>83.9</td><td>90.2</td><td><strong>92.2</strong></td></tr>
<tr><td>IF-Eval (Prompt Strict)</td><td><strong>86.5</strong></td><td>84.3</td><td>86.1</td><td>84.8</td><td>-</td><td>83.3</td></tr>
<tr><td>GFQA Diamond (Pass@1)</td><td>65.0</td><td>49.9</td><td>59.1</td><td>60.0</td><td><strong>75.7</strong></td><td>71.5</td></tr>
<tr><td>SimpleQA (Correct)</td><td>28.4</td><td>38.2</td><td>24.9</td><td>7.0</td><td><strong>47.0</strong></td><td>30.1</td></tr>
<tr><td>FRAMES (Acc.)</td><td>72.5</td><td>80.5</td><td>73.3</td><td>76.9</td><td>-</td><td><strong>82.5</strong></td></tr>
<tr><td>AlpacaEval2.0 (LC-winrate)</td><td>52.0</td><td>51.1</td><td>70.0</td><td>57.8</td><td>-</td><td><strong>87.6</strong></td></tr>
<tr><td>ArenaHard (GPT-4-1106)</td><td>85.2</td><td>80.4</td><td>85.5</td><td>92.0</td><td>-</td><td><strong>92.3</strong></td></tr>
<tr><td>LiveCodeBench (Pass@1-COT)</td><td>38.9</td><td>32.9</td><td>36.2</td><td>53.8</td><td>63.4</td><td><strong>65.9</strong></td></tr>
<tr><td>Codeforces (Percentile)</td><td>20.3</td><td>23.6</td><td>58.7</td><td>93.4</td><td><strong>96.6</strong></td><td>96.3</td></tr>
<tr><td>Codeforces (Rating)</td><td>717</td><td>759</td><td>1134</td><td>1820</td><td><strong>2061</strong></td><td>2029</td></tr>
<tr><td>SWE Verified (Resolved)</td><td><strong>50.8</strong></td><td>38.8</td><td>42.0</td><td>41.6</td><td>48.9</td><td>49.2</td></tr>
<tr><td>Aider-Polyglot (Acc.)</td><td>45.3</td><td>16.0</td><td>49.6</td><td>32.9</td><td><strong>61.7</strong></td><td>53.3</td></tr>
<tr><td>AIME 2024 (Pass@1)</td><td>16.0</td><td>9.3</td><td>39.2</td><td>63.6</td><td>79.2</td><td><strong>79.8</strong></td></tr>
<tr><td>MATH-500 (Pass@1)</td><td>78.3</td><td>74.6</td><td>90.2</td><td>90.0</td><td>96.4</td><td><strong>97.3</strong></td></tr>
<tr><td>CNMO 2024 (Pass@1)</td><td>13.1</td><td>10.8</td><td>43.2</td><td>67.6</td><td>-</td><td><strong>78.8</strong></td></tr>
<tr><td>CLUEWSC (EM)</td><td>85.4</td><td>87.9</td><td>90.9</td><td>89.9</td><td>-</td><td><strong>92.8</strong></td></tr>
<tr><td>C-Eval (EM)</td><td>76.7</td><td>76.0</td><td>86.5</td><td>68.9</td><td>-</td><td><strong>91.8</strong></td></tr>
<tr><td>C-SimpleQA (Correct)</td><td>55.4</td><td>58.7</td><td><strong>68.0</strong></td><td>40.3</td><td>-</td><td>63.7</td></tr>
</tbody></table>
</div><!-- markdownlint-enable MD013 -->
<div
  class="table-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
<p>Table: Comparison between DeepSeek-R1 and other representative models.
(Copied from Table 4 of Guo, Daya, et al (2025).)</p>
</div>
<ol>
<li>
<p><strong>Performance on Benchmarks:</strong> The table above which was copied from the DeepSeek-R1
paper compares the performance of DeepSeek-R1 and -V3 with representative models
from Anthropic and OpenAI. The values reported clearly demonstrate the impressive
performance of DeepSeek-R1 across various benchmarks and tasks. Most notably,
DeepSeek-R1 was able to surpass OpenAI's reasoning model o1-1217 on several benchmarks.</p>
</li>
<li>
<p><strong>Distilling Reasoning Capabilities:</strong> The 800K samples that included generated
examples by both DeepSeek-R1 (reasoning) and DeepSeek-V3 (non-reasoning) were
used to distill other open-source models like <a href="models/../models/qwen2pt5.html">Qwen</a>
and <a href="models/../models/llama_3.html">Llama</a>. With only the application SFT (i.e., no RL),
some of these distilled models were not only able to outperform OpenAI's non-reasoning
model GPT-4o-0513 across all benchmarks tested, but also OpenAI's o1-mini model
on most benchmarks.</p>
</li>
<li>
<p><strong>RL's Potential:</strong> Pure RL empowered DeepSeek-R1-Zero to autonomously acquire
robust reasoning capabilities without any SFT data. What's more is that as test-time
computation was increased, desirable behaviours such as reflection and re-evaluation
on past trajectories emerged making it possible for the model to have "aha moments"
when solving complex tasks. This development should serve as a reminder of the
great potential of RL and its overall place in AI as endeavour to reach new
heights.</p>
</li>
</ol>
<h2 id="limitations-3"><a class="header" href="#limitations-3">Limitations</a></h2>
<p>DeepSeek reported various limitations for DeepSeek-R1. Most notably, DeepSeek-R1
is inferior to DeepSeek-V3 in general capabilities such as function calling, producing
structured outputs (JSON), role-playing, and multi-turn conversations. Additionally,
due to its optimization for English and Chinese, the model sometimes suffers from
language mixing. Lastly, DeepSeek-R1 reportedly demonstrated a high sensitivity
to prompts and long inference times, making it unsuitable for low-latency applications
such as software-engineering tasks.</p>
<h4 id="references--useful-links-3"><a class="header" href="#references--useful-links-3">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2501.12948"><em>Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms
via reinforcement learning." arXiv preprint arXiv:2501.12948 (2025).</em></a></li>
<li><a href="https://arxiv.org/pdf/2412.19437"><em>Liu, Aixin, et al. "Deepseek-v3 technical report." arXiv preprint
arXiv:2412.19437 (2024).</em></a></li>
<li><a href="https://fortune.com/2025/01/27/china-deepseek-nvidia-gpu-investor-panic-us-export-controls-rethink/"><em>China's DeepSeek sets off Nvidia investor panic over US export controls</em></a>
<em>(appearing in fortune.com)</em></li>
<li><a href="https://huggingface.co/blog/open-r1"><em>Open-R1: a fully open reproduction of DeepSeek-R1</em></a>
<em>(by HuggingFace)</em></li>
<li><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"><em>DeepSeek-R1 is available on HuggingFace</em></a></li>
</ol>
<!-- markdownlint-disable-file MD033 -->
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable-file MD033 -->
<h1 id="deepseek-v3"><a class="header" href="#deepseek-v3">DeepSeek-v3</a></h1>
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 2em;">
  <div>
    <a target="_blank" href="https://github.com/VectorInstitute/ai-pocket-reference/issues/new?template=edit-request.yml">
      <img src="https://img.shields.io/badge/Suggest_an_Edit-black?logo=github&style=flat" alt="Suggest an Edit"/>
    </a>
    <p style="margin: 0;"><small>Reading time: 7 min</small></p>
  </div>
</div>
<p>The DeepSeek-V3 model was introduced by DeepSeek in December of 2024. It is an
LLM that leverages <a href="models/../llms/architecture/moe.html">MoE</a> in its design.</p>
<center>
<img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/deepseek-v3-lineage-v2.excalidraw.svg" alt="DeepSeek-V3 Model Lineage"> <!-- markdownlint-disable-line MD013 -->
</center>
<div
  class="figure-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Figure: Illustrating DeepSeek-V3 training evolution.
</div>
<p>The training pipeline for DeepSeek-V3 consists of the two typical stages: pre-training
and post-training. As depicted in the Figure above, the pre-training stage involves
pre-training on 14.8T tokens followed by long-context extension using the <a href="models/../llms/fine_tuning/yarn.html">YaRN</a>
methodology. Post-training of DeepSeek-V3 utilizes <a href="models/../llms/fine_tuning/sft.html">SFT</a>
as well as Reinforcement Learning methods.</p>
<h2 id="historical-significance-1"><a class="header" href="#historical-significance-1">Historical Significance</a></h2>
<p>At the time of its release, open-source models had already been lessening the gap
in performance with closed-source counterparts. DeepSeek-V3 was yet another open-source
model that achieved high levels of performance, beating other open-source alternatives
as well as some closed-source models in various benchmarks. What made DeepSeek-V3's
achievement even more intriguing was that it was reportedly trained using less
compute than its closest counterparts.</p>
<h2 id="architectural-highlights-1"><a class="header" href="#architectural-highlights-1">Architectural Highlights</a></h2>
<p>DeepSeek-V3 is a transformer-based model that swaps out nearly all dense <a href="models/../llms/architecture/feedforward.html">feedforward</a>
for <a href="models/../llms/architecture/moe.html">MoE</a>. The model has a total of 671B parameters
but through its specialized variant of MoE (referred to as DeepSeekMoE), only
37B parameters are activated in both training and inference. Through a series of
long-context extension fine-tuning steps, the maximum context length for this model
was extended to 128K tokens.</p>
<p><strong>DeepSeekMoE:</strong> Used to carry out training more efficiently, this MoE design
consists of two sets of experts, namely: shared and routed. The former set of routers
is used for every token in the input sequence whereas the usage of routed ones are
determined according to the affinity to the input token.</p>
<p><strong>Auxiliary-loss Load Free Balancing:</strong> When using an MoE architecture, one must
consider load balancing across the networks to prevent routing collapse. This has
been typically addressed via the introduction of an auxiliary loss. However, if
this loss has too great of an influence, it can lead to a model degradation. DeepSeek-V3
instead considers a technique that requires no auxiliary loss but instead relies
on a new bias term that dynamically changes its value according to the experts
current workload.</p>
<p><strong>Multi-Head Latent Attention (MLA):</strong> Used for making inference more efficient
by jointly compressing attention keys and values to a lower dimension. The compression
involves a linear projection matrix compressing keys and values down as well as
another linear project matrix for compressing keys and values back up. Only the
compressed joint representation of keys and values need to be cached during inference.
For more details see <a href="models/../llms/architecture/mla.html">MLA</a>.</p>
<p><strong>Multi-Token Prediction:</strong> In an effort to improve the training signal, DeepSeek-V3
expands the prediction scope to additional future tokens at every token position
of the sequence. In other words, instead of only predicting the next immediate token
and training the model on this signal, $D$ future tokens are predicted. These tokens
are predicted sequentially by $D$ sequential multi-token prediction modules in order
to maintain the causal chain. For more details see <a href="models/../llms/decoding/multi_token_prediction.html">MTP</a>.</p>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Value</th></tr></thead><tbody>
<tr><td>Total parameters</td><td>671B</td></tr>
<tr><td>Activated parameters</td><td>37B</td></tr>
<tr><td>Maximum context length</td><td>128K tokens</td></tr>
<tr><td>Number of Transformer layers</td><td>61</td></tr>
<tr><td>Hidden dimension size</td><td>7168</td></tr>
<tr><td>Number of attention heads</td><td>128</td></tr>
<tr><td>Number of experts (MoE)</td><td>1 (shared) &amp; 256 (routed)</td></tr>
<tr><td>Hidden dimension of experts</td><td>2048</td></tr>
<tr><td>KV compression dimension size (MLA)</td><td>512</td></tr>
<tr><td>Multi-token depth (MTP)</td><td>1</td></tr>
</tbody></table>
</div><div
  class="table-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Table 1: Summary of DeepSeek-V3 architecture and hyper parameters.
</div>
<h2 id="training-data-1"><a class="header" href="#training-data-1">Training Data</a></h2>
<p>The pre-training corpus is a revised version of the one used to train an earlier
version of the model, DeepSeek-V2. In this revision, more samples pertaining to
mathematics and programming were included. Ultimately, the dataset comprised of
14.8T tokens.</p>
<h2 id="compute-details"><a class="header" href="#compute-details">Compute Details</a></h2>
<p>DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs. Each node within
the cluster consists of 8 H800 GPUs inter-connected via NVLink and NVSwitch. In total,
it was reported that only 2.664M H800 GPU hours were used for pre-training while
subsequent training stages required only 0.1M GPU hours. One of the main reasons
for this training efficiency was their application of an FP8 mixed precision
training framework.</p>
<h2 id="key-results-1"><a class="header" href="#key-results-1">Key Results</a></h2>
<!-- markdownlint-disable MD013 -->
<div class="table-wrapper"><table><thead><tr><th>Benchmark (Metric)</th><th># Shots</th><th>DeepSeek-V2 Base</th><th>Qwen2.5 72B Base</th><th>LLaMA-3.1 405B Base</th><th>DeepSeek-V3 Base</th></tr></thead><tbody>
<tr><td>Pile-test (BPB)</td><td>-</td><td>0.606</td><td>0.638</td><td><strong>0.542</strong></td><td>0.548</td></tr>
<tr><td>BBH (EM)</td><td>3-shot</td><td>78.8</td><td>79.8</td><td>82.9</td><td><strong>87.5</strong></td></tr>
<tr><td>MMLU (EM)</td><td>5-shot</td><td>78.4</td><td>85.0</td><td>84.4</td><td><strong>87.1</strong></td></tr>
<tr><td>MMLU-Redux (EM)</td><td>5-shot</td><td>75.6</td><td>83.2</td><td>81.3</td><td><strong>86.2</strong></td></tr>
<tr><td>MMLU-Pro (EM)</td><td>5-shot</td><td>51.4</td><td>58.3</td><td>52.8</td><td><strong>64.4</strong></td></tr>
<tr><td>DROP (F1)</td><td>3-shot</td><td>80.4</td><td>80.6</td><td>86.0</td><td><strong>89.0</strong></td></tr>
<tr><td>ARC-Easy (EM)</td><td>25-shot</td><td>97.6</td><td>98.4</td><td>98.4</td><td><strong>98.9</strong></td></tr>
<tr><td>ARC-Challenge (EM)</td><td>25-shot</td><td>92.2</td><td>94.5</td><td><strong>95.3</strong></td><td><strong>95.3</strong></td></tr>
<tr><td>HellaSwag (EM)</td><td>10-shot</td><td>87.1</td><td>84.8</td><td><strong>89.2</strong></td><td>88.9</td></tr>
<tr><td>PIQA (EM)</td><td>0-shot</td><td>83.9</td><td>82.1</td><td><strong>85.9</strong></td><td>84.7</td></tr>
<tr><td>WinoGrande (EM)</td><td>5-shot</td><td><strong>86.3</strong></td><td>82.3</td><td>85.2</td><td>84.9</td></tr>
<tr><td>RACE-Middle (EM)</td><td>3-shot</td><td>73.1</td><td>68.1</td><td><strong>74.2</strong></td><td>74.9</td></tr>
<tr><td>RACE-High (EM)</td><td>5-shot</td><td>52.6</td><td>50.3</td><td><strong>56.8</strong></td><td>51.3</td></tr>
<tr><td>TriviaQA (EM)</td><td>5-shot</td><td>80.0</td><td>71.9</td><td><strong>82.7</strong></td><td>82.9</td></tr>
<tr><td>NaturalQuestions (EM)</td><td>5-shot</td><td>38.6</td><td>33.2</td><td><strong>41.5</strong></td><td>40.0</td></tr>
<tr><td>AGIEval (EM)</td><td>0-shot</td><td>57.5</td><td>75.8</td><td>60.6</td><td><strong>79.6</strong></td></tr>
<tr><td>HumanEval (Pass@1)</td><td>0-shot</td><td>43.3</td><td>53.0</td><td>54.9</td><td><strong>65.2</strong></td></tr>
<tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>65.0</td><td>72.6</td><td>68.4</td><td><strong>75.4</strong></td></tr>
<tr><td>LiveCodeBench-Base (Pass@1)</td><td>3-shot</td><td>11.6</td><td>12.9</td><td>15.1</td><td><strong>19.4</strong></td></tr>
<tr><td>CRUXEval-1 (EM)</td><td>2-shot</td><td>52.5</td><td>59.1</td><td>58.5</td><td><strong>67.3</strong></td></tr>
<tr><td>CRUXEval-O (EM)</td><td>2-shot</td><td>49.8</td><td>59.9</td><td>59.9</td><td><strong>69.8</strong></td></tr>
<tr><td>CSMRR (EM)</td><td>8-shot</td><td>81.6</td><td>88.3</td><td>89.3</td><td><strong>89.3</strong></td></tr>
<tr><td>MATH (EM)</td><td>4-shot</td><td>43.4</td><td>54.4</td><td>49.0</td><td><strong>61.6</strong></td></tr>
<tr><td>MGSM (EM)</td><td>8-shot</td><td>63.6</td><td>76.2</td><td>69.9</td><td><strong>79.8</strong></td></tr>
<tr><td>CMath (EM)</td><td>3-shot</td><td>78.7</td><td>84.5</td><td>77.3</td><td><strong>90.7</strong></td></tr>
<tr><td>CLUEWSC (EM)</td><td>5-shot</td><td>82.0</td><td>82.5</td><td><strong>83.0</strong></td><td>82.7</td></tr>
<tr><td>C-Eval (EM)</td><td>0-shot</td><td>81.4</td><td>72.5</td><td>72.5</td><td><strong>90.1</strong></td></tr>
<tr><td>CMMLU (EM)</td><td>5-shot</td><td>84.0</td><td><strong>89.5</strong></td><td>73.7</td><td>88.8</td></tr>
<tr><td>CMRC (EM)</td><td>1-shot</td><td><strong>77.4</strong></td><td>75.8</td><td>76.0</td><td>76.3</td></tr>
<tr><td>C3 (EM)</td><td>0-shot</td><td>77.4</td><td>76.7</td><td><strong>79.7</strong></td><td>78.6</td></tr>
<tr><td>CCPM (EM)</td><td>0-shot</td><td><strong>93.0</strong></td><td>88.5</td><td>78.6</td><td>92.0</td></tr>
<tr><td>MMLU-non-English (EM)</td><td>5-shot</td><td>64.0</td><td>74.8</td><td>73.8</td><td><strong>79.4</strong></td></tr>
</tbody></table>
</div><!-- markdownlint-enable MD013 -->
<div
  class="table-caption"
  style="text-align: center; font-size: 0.8em; margin-top: 10px;"
>
Table 2: Comparison between DeepSeek-V3 and other representative models.
(Copied from Table 3 of Liu, Aixin, et al (2024).)
</div>
<ol>
<li>
<p><strong>Superior Open-Source Model:</strong> DeepSeek-V3 outperformed all other open-source
models on educational benchmarks (MMLU, MMLU-Pro, GPQA) achieving performance
levels that rivals that for closed-source models such as GPT-4o and Claude-Sonnet-3.5.
DeepSeek-V3 also achieved SOTA on math-related benchmarks (GSM8K, MATH, MGSM,
CMath).</p>
</li>
<li>
<p><strong>Efficient Training:</strong> DeepSeek-V3 was trained using only 2.664M H800 GPU hours,
leveraging an FP8 mixed precision training framework. This marked, as reported
by the authors, the first successful use of an FP8 scheme to train a large-scale
model.</p>
</li>
<li>
<p><strong>Reasoning Distillation:</strong> As part of the post-training step, DeepSeek-V3 creators
were able to distill reasoning capabilities via long <a href="models/../llms/prompting/cot.html">CoT</a>
passages generated by <a href="models/../models/deepseek_r1.html">DeepSeek-R1</a>. The authors noted
that this pipeline improved reasononing performance while still maintaining the
ability to produce desired outputs and efficient response lengths.</p>
</li>
</ol>
<h2 id="limitations-4"><a class="header" href="#limitations-4">Limitations</a></h2>
<p>DeepSeek-V3 requires significant amounts of computation facilities to ensure efficient
inference.</p>
<h4 id="references--useful-links-4"><a class="header" href="#references--useful-links-4">References &amp; Useful Links <!-- markdownlint-disable-line MD001 --></a></h4>
<ol>
<li><a href="https://arxiv.org/pdf/2412.19437"><em>Liu, Aixin, et al. "Deepseek-v3 technical report." arXiv preprint
arXiv:2412.19437 (2024).</em></a></li>
<li><a href="https://www.reuters.com/technology/chinas-deepseek-sets-off-ai-market-rout-2025-01-27/">DeepSeek sparks AI stock selloff; Nvidia posts record market-cap loss</a>
(<em>appearing in reuters.com</em>)</li>
</ol>
<!-- markdownlint-disable-file MD033 -->
<hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
<div class="contributor-footnotes">
<small>
<strong>Contributors:</strong>
<div style="display: flex; gap: 8px; margin-top: 10px;">
<a href="https://github.com/nerdai">
<img src="https://github.com/nerdai.png"
  width="32px" alt="Contributor nerdai" style="border-radius: 50%">
</a>
</div>
</small>
</div>
<div class="vector-logo">
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-default.png" alt="" class="light-logo">
    </a>
    <a href="https://vectorinstitute.ai/">
        <img src="https://d3ddy8balm3goa.cloudfront.net/vector-ai-pocket-refs/vector-logo-dark.png" alt="" class="dark-logo">
    </a>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
